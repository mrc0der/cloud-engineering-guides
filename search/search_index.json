{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Engineering Labs Platforms Covered AWS Azure Technologies covered Databases CICD Containers Immutable Consulting Tips Migration Discovery Workload Discovery","title":"Cloud Engineering Labs"},{"location":"#cloud-engineering-labs","text":"","title":"Cloud Engineering Labs"},{"location":"#platforms-covered","text":"AWS Azure","title":"Platforms Covered"},{"location":"#technologies-covered","text":"Databases CICD Containers Immutable","title":"Technologies covered"},{"location":"#consulting-tips","text":"Migration Discovery Workload Discovery","title":"Consulting Tips"},{"location":"AWS/","text":"Getting started with AWS AWS 100 Labs AWS 200 Labs Ec2 User Data","title":"Getting started with AWS"},{"location":"AWS/#getting-started-with-aws","text":"AWS 100 Labs AWS 200 Labs Ec2 User Data","title":"Getting started with AWS"},{"location":"Azure/","text":"Azure Azure Azure 100 Labs Azure 200 Labs","title":"Azure"},{"location":"Azure/#azure","text":"Azure Azure 100 Labs Azure 200 Labs","title":"Azure"},{"location":"CICD/","text":"CI/CD CICD Labs Deployment Strategies In-Place Deployment Pros: Simplicity: Straightforward to implement and understand, requiring minimal infrastructure changes. Cost-effective: Does not require additional resources for parallel environments. Familiarity: Commonly used, making it well-understood by many teams. Cons: Downtime: Likely to incur some downtime during deployment, affecting availability. Riskier: Limited scope for testing in production-like environments, increasing the risk of deployment issues. Rollback complexity: Rolling back a deployment can be complex and time-consuming. Blue/Green Deployment Pros: Zero downtime: Allows for live environment swapping, ensuring no service disruption. Instant rollback: Easy and quick to revert to the previous version if issues arise. Testing in production: Facilitates final testing in a production-like environment before going live. Cons: Cost: Requires double the environment resources, increasing infrastructure costs. Complexity: More complex setup and management compared to in-place deployments. Synchronization: Requires careful synchronization of databases and shared resources during the switch. Canary Releases Pros: Risk mitigation: Gradual rollout reduces the impact of errors on the entire user base. Real user feedback: Enables gathering real-world usage feedback and performance metrics. Scalable: Adjust the user percentage based on feedback and performance data. Cons: Management overhead: More complex to manage different versions among users. Metric analysis: Requires sophisticated monitoring and analysis tools to measure impact accurately. Rollout duration: Can take longer to fully deploy a feature or fix to all users. A/B Testing Deployment Pros: Data-driven decisions: Direct comparison of different versions to make informed decisions based on user interaction. User-centric: Focuses on improving user experience and engagement. Incremental improvement: Allows for continuous improvement of features based on user feedback. Cons: Complexity: Requires infrastructure to segment users and serve different versions. Analysis resources: Needs significant resources for data collection, monitoring, and analysis. Scope limitation: Typically limited to user-facing features, not suitable for backend changes. Automated Testing Integration Pros Quality assurance: Ensures code changes meet quality standards before deployment. Early bug detection: Identifies and addresses issues early in the development cycle. Confidence: Increases confidence in the stability of the application with each release. Cons Setup time: Requires time and effort to set up comprehensive test suites initially. Maintenance: Test suites need regular updates to stay relevant to the application's evolution. False positives/negatives: Risk of tests not accurately reflecting real-world usage or issues. Infrastructure as Code (IaC) for CI/CD Setup Pros Consistency: Ensures infrastructure provisioning consistency and reduces manual errors. Speed: Accelerates the setup and scaling of environments. Version control: Infrastructure changes can be versioned and tracked like application code. Cons Learning curve: Requires understanding IaC syntax and best practices. Initial setup: Takes time to define and test infrastructure code initially. Complexity: Managing and organizing IaC for large-scale environments can be complex. Tools GitHub Repo Tips https://github.com/joelparkerhenderson/github-special-files-and-paths Games https://github.com/joshzcold/Cold-Family-Feud Actions GitHub Action for pylint - GitHub Marketplace Automating Dependabot with GitHub Actions - GitHub Docs","title":"CI/CD"},{"location":"CICD/#cicd","text":"CICD Labs","title":"CI/CD"},{"location":"CICD/#deployment-strategies","text":"","title":"Deployment Strategies"},{"location":"CICD/#in-place-deployment","text":"Pros: Simplicity: Straightforward to implement and understand, requiring minimal infrastructure changes. Cost-effective: Does not require additional resources for parallel environments. Familiarity: Commonly used, making it well-understood by many teams. Cons: Downtime: Likely to incur some downtime during deployment, affecting availability. Riskier: Limited scope for testing in production-like environments, increasing the risk of deployment issues. Rollback complexity: Rolling back a deployment can be complex and time-consuming.","title":"In-Place Deployment"},{"location":"CICD/#bluegreen-deployment","text":"Pros: Zero downtime: Allows for live environment swapping, ensuring no service disruption. Instant rollback: Easy and quick to revert to the previous version if issues arise. Testing in production: Facilitates final testing in a production-like environment before going live. Cons: Cost: Requires double the environment resources, increasing infrastructure costs. Complexity: More complex setup and management compared to in-place deployments. Synchronization: Requires careful synchronization of databases and shared resources during the switch.","title":"Blue/Green Deployment"},{"location":"CICD/#canary-releases","text":"Pros: Risk mitigation: Gradual rollout reduces the impact of errors on the entire user base. Real user feedback: Enables gathering real-world usage feedback and performance metrics. Scalable: Adjust the user percentage based on feedback and performance data. Cons: Management overhead: More complex to manage different versions among users. Metric analysis: Requires sophisticated monitoring and analysis tools to measure impact accurately. Rollout duration: Can take longer to fully deploy a feature or fix to all users.","title":"Canary Releases"},{"location":"CICD/#ab-testing-deployment","text":"Pros: Data-driven decisions: Direct comparison of different versions to make informed decisions based on user interaction. User-centric: Focuses on improving user experience and engagement. Incremental improvement: Allows for continuous improvement of features based on user feedback. Cons: Complexity: Requires infrastructure to segment users and serve different versions. Analysis resources: Needs significant resources for data collection, monitoring, and analysis. Scope limitation: Typically limited to user-facing features, not suitable for backend changes.","title":"A/B Testing Deployment"},{"location":"CICD/#automated-testing-integration","text":"","title":"Automated Testing Integration"},{"location":"CICD/#pros","text":"Quality assurance: Ensures code changes meet quality standards before deployment. Early bug detection: Identifies and addresses issues early in the development cycle. Confidence: Increases confidence in the stability of the application with each release.","title":"Pros"},{"location":"CICD/#cons","text":"Setup time: Requires time and effort to set up comprehensive test suites initially. Maintenance: Test suites need regular updates to stay relevant to the application's evolution. False positives/negatives: Risk of tests not accurately reflecting real-world usage or issues.","title":"Cons"},{"location":"CICD/#infrastructure-as-code-iac-for-cicd-setup","text":"","title":"Infrastructure as Code (IaC) for CI/CD Setup"},{"location":"CICD/#pros_1","text":"Consistency: Ensures infrastructure provisioning consistency and reduces manual errors. Speed: Accelerates the setup and scaling of environments. Version control: Infrastructure changes can be versioned and tracked like application code.","title":"Pros"},{"location":"CICD/#cons_1","text":"Learning curve: Requires understanding IaC syntax and best practices. Initial setup: Takes time to define and test infrastructure code initially. Complexity: Managing and organizing IaC for large-scale environments can be complex.","title":"Cons"},{"location":"CICD/#tools","text":"","title":"Tools"},{"location":"CICD/#github","text":"","title":"GitHub"},{"location":"CICD/#repo-tips","text":"https://github.com/joelparkerhenderson/github-special-files-and-paths","title":"Repo Tips"},{"location":"CICD/#games","text":"https://github.com/joshzcold/Cold-Family-Feud","title":"Games"},{"location":"CICD/#actions","text":"GitHub Action for pylint - GitHub Marketplace Automating Dependabot with GitHub Actions - GitHub Docs","title":"Actions"},{"location":"Containers/","text":"Containers Labs Docker 100 Kube Labs Containers Tools Helm Kubernetes Integration: Designed specifically for Kubernetes, providing an efficient way to package, configure, and deploy applications. Templated: Offers templated capabilities for managing Kubernetes manifests across different environments. Community: A rich ecosystem of pre-built charts for popular software, facilitating easy deployment.","title":"Containers"},{"location":"Containers/#containers","text":"","title":"Containers"},{"location":"Containers/#labs","text":"Docker 100 Kube Labs","title":"Labs"},{"location":"Containers/#containers-tools","text":"","title":"Containers Tools"},{"location":"Containers/#helm","text":"Kubernetes Integration: Designed specifically for Kubernetes, providing an efficient way to package, configure, and deploy applications. Templated: Offers templated capabilities for managing Kubernetes manifests across different environments. Community: A rich ecosystem of pre-built charts for popular software, facilitating easy deployment.","title":"Helm"},{"location":"Databases/","text":"Databases MySQL Use Cases Web applications: MySQL is often used as the backend database for web applications due to its reliability and ease of integration with web technologies. E-commerce: It supports complex transactions and inventory management systems. Content Management Systems (CMS) like WordPress, Joomla, and Drupal. Pros Widely Used and Supported: MySQL is one of the most popular relational database management systems (RDBMS), ensuring a vast community and numerous resources. Reliable: It is known for its reliability and robustness. Compatibility: MySQL is compatible with all major hosting platforms, making it easy to deploy. Cons Scalability: While MySQL can handle a lot of concurrent connections, it might struggle with extremely large databases or high write-load applications compared to some NoSQL databases. Complexity in High Availability: Setting up a highly available MySQL cluster can be more complex compared to some NoSQL counterparts. Best Practices Regularly backup your databases and test the restore procedure to ensure data integrity. Use InnoDB storage engine for transactions and data integrity. Optimize your queries and indexes for better performance. MongoDB Use Cases Big Data Applications: MongoDB handles large volumes of data and is suitable for big data applications. Mobile and Social Networking Applications: It's ideal for applications that require rapid development, as well as flexible and agile sprints. Document Storage: Suited for applications that require a flexible schema and the ability to store nested data structures like JSON documents. Pros Schema-less: MongoDB is a NoSQL database that allows you to store - documents without a predefined schema. Scalability: It offers horizontal scalability through sharding. Flexibility: The flexible schema allows for easy modifications of data structure. Cons Data Size: The size of the data can grow significantly due to its document-based structure. Transactions: While MongoDB supports transactions, they might not be as mature or straightforward as in traditional RDBMS like MySQL. Best Practices Design your schema according to your application's access patterns. Use indexes effectively to speed up query performance. Monitor your MongoDB clusters and optimize them regularly. Elasticsearch Use Cases Full-Text Search: Ideal for applications requiring complex search capabilities, like e-commerce sites or content repositories. Log and Event Data Analysis: Commonly used for storing, searching, and analyzing large volumes of log data. Real-Time Analytics: Suitable for applications requiring real-time analysis of data. Pros Fast Search: Built on Lucene, Elasticsearch provides fast full-text search capabilities. Scalable: It can easily scale horizontally to handle large volumes of data. Flexible: Supports complex query types and analytics. Cons Complexity: Can be complex to set up and maintain, especially in larger clusters. Resource Intensive: Can be resource-intensive, requiring significant memory and CPU. Best Practices Plan your index strategy carefully (e.g., sharding and replication strategies). Monitor cluster health and performance actively. Secure your Elasticsearch cluster, especially if it's exposed to the internet. InfluxDB Use Cases Time-Series Data: InfluxDB is specifically designed for time-series data, making it suitable for IoT applications, monitoring systems, and real-time analytics. Metrics Collection: Commonly used for storing application performance monitoring (APM) data or network performance data. Pros Optimized for Time-Series: High performance and efficiency for time-series data. Simple Query Language: Uses a SQL-like query language that's easy to learn. Built-in HTTP API: Facilitates easy integration with various data sources and applications. Cons Limited to Time-Series: Not suitable for general-purpose data storage. Scalability: Clustering and high availability setups can be more complex compared to some other databases. Best Practices Structure your data to leverage InfluxDB's strengths in time-series data storage. Use continuous queries for downsampling data to optimize storage. Regularly monitor and tune your InfluxDB for optimal performance. Prometheus Use Cases Monitoring and Alerting: Prometheus is primarily used for IT infrastructure monitoring and alerting based on time-series data. System Metrics Collection: Collects and stores metrics as time-series data, making it suitable for observing the health of IT infrastructure. Pros Multidimensional Data Model: Uses a powerful data model and a query language that can handle complex queries efficiently. Active Community: Supported by a large community and a part of the Cloud Native Computing Foundation (CNCF). Integrated Alerting: Comes with an in-built alerting mechanism, Alertmanager. Cons Data Retention: By default, it does not handle long-term metric storage. Complexity in High Availability Setup: Setting up a highly available Prometheus can be complex. Best Practices Use exporters and integrations wisely to gather metrics from your systems and services. Regularly review and optimize your alerting rules to reduce noise. Consider integrating with long-term storage solutions if needed for your use case. Time-Series Databases (General) Use Cases IoT and Sensor Data: Ideal for storing and analyzing data from sensors and IoT devices over time. Financial Data: Used for storing stock prices, currency exchange rates, and financial transactions. Pros Efficient Storage and Retrieval: Optimized for the storage, retrieval, and real-time analysis of time-series data. Trend Analysis: Ideal for applications requiring trend analysis over time. Cons Specialization: Primarily focused on time-series data, which might not be suitable for all types of data. Query Complexity: Queries can become complex, especially when dealing with large datasets and long time ranges. Best Practices Choose the right granularity for your data points to balance between detail and storage requirements. Use downsampling to reduce data size and maintain performance. Consider data retention policies to manage data growth efficiently.","title":"Databases"},{"location":"Databases/#databases","text":"","title":"Databases"},{"location":"Databases/#mysql","text":"","title":"MySQL"},{"location":"Databases/#use-cases","text":"Web applications: MySQL is often used as the backend database for web applications due to its reliability and ease of integration with web technologies. E-commerce: It supports complex transactions and inventory management systems. Content Management Systems (CMS) like WordPress, Joomla, and Drupal.","title":"Use Cases"},{"location":"Databases/#pros","text":"Widely Used and Supported: MySQL is one of the most popular relational database management systems (RDBMS), ensuring a vast community and numerous resources. Reliable: It is known for its reliability and robustness. Compatibility: MySQL is compatible with all major hosting platforms, making it easy to deploy.","title":"Pros"},{"location":"Databases/#cons","text":"Scalability: While MySQL can handle a lot of concurrent connections, it might struggle with extremely large databases or high write-load applications compared to some NoSQL databases. Complexity in High Availability: Setting up a highly available MySQL cluster can be more complex compared to some NoSQL counterparts.","title":"Cons"},{"location":"Databases/#best-practices","text":"Regularly backup your databases and test the restore procedure to ensure data integrity. Use InnoDB storage engine for transactions and data integrity. Optimize your queries and indexes for better performance.","title":"Best Practices"},{"location":"Databases/#mongodb","text":"","title":"MongoDB"},{"location":"Databases/#use-cases_1","text":"Big Data Applications: MongoDB handles large volumes of data and is suitable for big data applications. Mobile and Social Networking Applications: It's ideal for applications that require rapid development, as well as flexible and agile sprints. Document Storage: Suited for applications that require a flexible schema and the ability to store nested data structures like JSON documents.","title":"Use Cases"},{"location":"Databases/#pros_1","text":"Schema-less: MongoDB is a NoSQL database that allows you to store - documents without a predefined schema. Scalability: It offers horizontal scalability through sharding. Flexibility: The flexible schema allows for easy modifications of data structure.","title":"Pros"},{"location":"Databases/#cons_1","text":"Data Size: The size of the data can grow significantly due to its document-based structure. Transactions: While MongoDB supports transactions, they might not be as mature or straightforward as in traditional RDBMS like MySQL.","title":"Cons"},{"location":"Databases/#best-practices_1","text":"Design your schema according to your application's access patterns. Use indexes effectively to speed up query performance. Monitor your MongoDB clusters and optimize them regularly.","title":"Best Practices"},{"location":"Databases/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"Databases/#use-cases_2","text":"Full-Text Search: Ideal for applications requiring complex search capabilities, like e-commerce sites or content repositories. Log and Event Data Analysis: Commonly used for storing, searching, and analyzing large volumes of log data. Real-Time Analytics: Suitable for applications requiring real-time analysis of data.","title":"Use Cases"},{"location":"Databases/#pros_2","text":"Fast Search: Built on Lucene, Elasticsearch provides fast full-text search capabilities. Scalable: It can easily scale horizontally to handle large volumes of data. Flexible: Supports complex query types and analytics.","title":"Pros"},{"location":"Databases/#cons_2","text":"Complexity: Can be complex to set up and maintain, especially in larger clusters. Resource Intensive: Can be resource-intensive, requiring significant memory and CPU.","title":"Cons"},{"location":"Databases/#best-practices_2","text":"Plan your index strategy carefully (e.g., sharding and replication strategies). Monitor cluster health and performance actively. Secure your Elasticsearch cluster, especially if it's exposed to the internet.","title":"Best Practices"},{"location":"Databases/#influxdb","text":"","title":"InfluxDB"},{"location":"Databases/#use-cases_3","text":"Time-Series Data: InfluxDB is specifically designed for time-series data, making it suitable for IoT applications, monitoring systems, and real-time analytics. Metrics Collection: Commonly used for storing application performance monitoring (APM) data or network performance data.","title":"Use Cases"},{"location":"Databases/#pros_3","text":"Optimized for Time-Series: High performance and efficiency for time-series data. Simple Query Language: Uses a SQL-like query language that's easy to learn. Built-in HTTP API: Facilitates easy integration with various data sources and applications.","title":"Pros"},{"location":"Databases/#cons_3","text":"Limited to Time-Series: Not suitable for general-purpose data storage. Scalability: Clustering and high availability setups can be more complex compared to some other databases.","title":"Cons"},{"location":"Databases/#best-practices_3","text":"Structure your data to leverage InfluxDB's strengths in time-series data storage. Use continuous queries for downsampling data to optimize storage. Regularly monitor and tune your InfluxDB for optimal performance.","title":"Best Practices"},{"location":"Databases/#prometheus","text":"","title":"Prometheus"},{"location":"Databases/#use-cases_4","text":"Monitoring and Alerting: Prometheus is primarily used for IT infrastructure monitoring and alerting based on time-series data. System Metrics Collection: Collects and stores metrics as time-series data, making it suitable for observing the health of IT infrastructure.","title":"Use Cases"},{"location":"Databases/#pros_4","text":"Multidimensional Data Model: Uses a powerful data model and a query language that can handle complex queries efficiently. Active Community: Supported by a large community and a part of the Cloud Native Computing Foundation (CNCF). Integrated Alerting: Comes with an in-built alerting mechanism, Alertmanager.","title":"Pros"},{"location":"Databases/#cons_4","text":"Data Retention: By default, it does not handle long-term metric storage. Complexity in High Availability Setup: Setting up a highly available Prometheus can be complex.","title":"Cons"},{"location":"Databases/#best-practices_4","text":"Use exporters and integrations wisely to gather metrics from your systems and services. Regularly review and optimize your alerting rules to reduce noise. Consider integrating with long-term storage solutions if needed for your use case.","title":"Best Practices"},{"location":"Databases/#time-series-databases-general","text":"","title":"Time-Series Databases (General)"},{"location":"Databases/#use-cases_5","text":"IoT and Sensor Data: Ideal for storing and analyzing data from sensors and IoT devices over time. Financial Data: Used for storing stock prices, currency exchange rates, and financial transactions.","title":"Use Cases"},{"location":"Databases/#pros_5","text":"Efficient Storage and Retrieval: Optimized for the storage, retrieval, and real-time analysis of time-series data. Trend Analysis: Ideal for applications requiring trend analysis over time.","title":"Pros"},{"location":"Databases/#cons_5","text":"Specialization: Primarily focused on time-series data, which might not be suitable for all types of data. Query Complexity: Queries can become complex, especially when dealing with large datasets and long time ranges.","title":"Cons"},{"location":"Databases/#best-practices_5","text":"Choose the right granularity for your data points to balance between detail and storage requirements. Use downsampling to reduce data size and maintain performance. Consider data retention policies to manage data growth efficiently.","title":"Best Practices"},{"location":"DevOps/","text":"DevOps DevOps Technical Skills Soft Skills Operational Skills Common Tools Ansible Pros Cons Packer Pros Cons Chef Pros Cons Technical Skills Coding and Scripting: Proficiency in at least one programming language (e.g., Python, Ruby, Java) and scripting skills for automation tasks. Infrastructure as Code (IaC): Experience with tools like Terraform, AWS CloudFormation, or Azure Resource Manager to manage infrastructure through code. Continuous Integration and Continuous Deployment (CI/CD): Understanding of CI/CD pipelines and tools such as Jenkins, GitLab CI, CircleCI, or GitHub Actions to automate the integration and deployment processes. Configuration Management: Familiarity with configuration management tools like Ansible, Puppet, or Chef to automate the provisioning and management of software. Containerization and Orchestration: Knowledge of container technologies like Docker and container orchestration tools such as Kubernetes or Docker Swarm. Cloud Services: Proficiency in at least one cloud platform (AWS, Azure, Google Cloud) and understanding of cloud-native services and architecture. Monitoring and Logging: Experience with tools like Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, Kibana), or Splunk for system monitoring and log management. Networking and Security : Basic understanding of network protocols, security best practices, firewalls, VPNs, and encryption technologies. Version Control: Proficiency with version control systems, especially Git, for source code management. Soft Skills Collaboration and Communication: Ability to work closely with development, operations, and other teams, facilitating a culture of open communication and collaborative problem-solving. Problem-Solving: Strong analytical and problem-solving skills to troubleshoot and resolve complex issues across various technologies and platforms. Adaptability: Willingness to continuously learn and adapt to new technologies and methodologies in the rapidly evolving DevOps landscape. Empathy and Understanding: Understanding the challenges and goals of both development and operations teams to create solutions that address the needs of both. Time Management and Prioritization: Ability to manage time effectively, prioritize tasks, and meet deadlines in a fast-paced environment. Operational Skills Agile and Lean Practices: Understanding of Agile software development methodologies and lean principles to improve efficiency and effectiveness in workflow processes. System Administration: Knowledge of system administration tasks for various operating systems (Linux, Windows) such as system setup, configuration, and maintenance. Disaster Recovery and Backup: Understanding of disaster recovery planning, backup strategies, and ensuring high availability of services. Performance Tuning: Ability to optimize system and application performance by tuning configurations and resources. Compliance and Security: Awareness of compliance requirements and security best practices to ensure that infrastructure and applications are secure from threats. Common Tools Ansible Pros Simplicity: Easy to learn with straightforward YAML syntax for defining automation tasks. Agentless: Requires no agents on the target nodes, reducing overhead and complexity. Extensibility: Rich library of modules for managing various infrastructure components. Cons Performance: Can be slower compared to agent-based tools, especially at scale. Limited Scope: Primarily focused on configuration management and orchestration, with less emphasis on state management. Error Handling: Complex playbooks can sometimes lead to less intuitive error handling. Resources: Ansible Official Documentation Packer Pros Automation: Automates the creation of machine and container images for multiple platforms from a single source configuration. Immutability: Encourages immutable infrastructure practices by creating machine images that are not changed after deployment. Integration: Integrates well with configuration management tools like Chef, Ansible, and Puppet for provisioning. Cons Learning Curve: Requires understanding of both the tool and the intricacies of building images across different platforms. Initial Setup: Setting up complex image builds can be time-consuming. Overhead: Additional overhead of managing image artifacts and versions. Resources: Packer Official Website Chef Pros Powerful: Offers robust capabilities for managing complex infrastructures with a mature Ruby-based DSL. Ecosystem: Strong community support with a large collection of \"cookbooks\" for common configurations. Enterprise Features: Provides a comprehensive suite of tools for enterprise users, including automated testing. Cons Complexity: Steeper learning curve due to Ruby DSL and the complexity of managing cookbooks. Bootstrapping: Requires an agent to be installed on managed nodes, which can add to the setup time. Management: Managing a large number of cookbooks and dependencies can become challenging. Resources: Chef Official Documentation","title":"DevOps"},{"location":"DevOps/#devops","text":"DevOps Technical Skills Soft Skills Operational Skills Common Tools Ansible Pros Cons Packer Pros Cons Chef Pros Cons","title":"DevOps"},{"location":"DevOps/#technical-skills","text":"Coding and Scripting: Proficiency in at least one programming language (e.g., Python, Ruby, Java) and scripting skills for automation tasks. Infrastructure as Code (IaC): Experience with tools like Terraform, AWS CloudFormation, or Azure Resource Manager to manage infrastructure through code. Continuous Integration and Continuous Deployment (CI/CD): Understanding of CI/CD pipelines and tools such as Jenkins, GitLab CI, CircleCI, or GitHub Actions to automate the integration and deployment processes. Configuration Management: Familiarity with configuration management tools like Ansible, Puppet, or Chef to automate the provisioning and management of software. Containerization and Orchestration: Knowledge of container technologies like Docker and container orchestration tools such as Kubernetes or Docker Swarm. Cloud Services: Proficiency in at least one cloud platform (AWS, Azure, Google Cloud) and understanding of cloud-native services and architecture. Monitoring and Logging: Experience with tools like Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, Kibana), or Splunk for system monitoring and log management. Networking and Security : Basic understanding of network protocols, security best practices, firewalls, VPNs, and encryption technologies. Version Control: Proficiency with version control systems, especially Git, for source code management.","title":"Technical Skills"},{"location":"DevOps/#soft-skills","text":"Collaboration and Communication: Ability to work closely with development, operations, and other teams, facilitating a culture of open communication and collaborative problem-solving. Problem-Solving: Strong analytical and problem-solving skills to troubleshoot and resolve complex issues across various technologies and platforms. Adaptability: Willingness to continuously learn and adapt to new technologies and methodologies in the rapidly evolving DevOps landscape. Empathy and Understanding: Understanding the challenges and goals of both development and operations teams to create solutions that address the needs of both. Time Management and Prioritization: Ability to manage time effectively, prioritize tasks, and meet deadlines in a fast-paced environment.","title":"Soft Skills"},{"location":"DevOps/#operational-skills","text":"Agile and Lean Practices: Understanding of Agile software development methodologies and lean principles to improve efficiency and effectiveness in workflow processes. System Administration: Knowledge of system administration tasks for various operating systems (Linux, Windows) such as system setup, configuration, and maintenance. Disaster Recovery and Backup: Understanding of disaster recovery planning, backup strategies, and ensuring high availability of services. Performance Tuning: Ability to optimize system and application performance by tuning configurations and resources. Compliance and Security: Awareness of compliance requirements and security best practices to ensure that infrastructure and applications are secure from threats.","title":"Operational Skills"},{"location":"DevOps/#common-tools","text":"","title":"Common Tools"},{"location":"DevOps/#ansible","text":"","title":"Ansible"},{"location":"DevOps/#pros","text":"Simplicity: Easy to learn with straightforward YAML syntax for defining automation tasks. Agentless: Requires no agents on the target nodes, reducing overhead and complexity. Extensibility: Rich library of modules for managing various infrastructure components.","title":"Pros"},{"location":"DevOps/#cons","text":"Performance: Can be slower compared to agent-based tools, especially at scale. Limited Scope: Primarily focused on configuration management and orchestration, with less emphasis on state management. Error Handling: Complex playbooks can sometimes lead to less intuitive error handling. Resources: Ansible Official Documentation","title":"Cons"},{"location":"DevOps/#packer","text":"","title":"Packer"},{"location":"DevOps/#pros_1","text":"Automation: Automates the creation of machine and container images for multiple platforms from a single source configuration. Immutability: Encourages immutable infrastructure practices by creating machine images that are not changed after deployment. Integration: Integrates well with configuration management tools like Chef, Ansible, and Puppet for provisioning.","title":"Pros"},{"location":"DevOps/#cons_1","text":"Learning Curve: Requires understanding of both the tool and the intricacies of building images across different platforms. Initial Setup: Setting up complex image builds can be time-consuming. Overhead: Additional overhead of managing image artifacts and versions. Resources: Packer Official Website","title":"Cons"},{"location":"DevOps/#chef","text":"","title":"Chef"},{"location":"DevOps/#pros_2","text":"Powerful: Offers robust capabilities for managing complex infrastructures with a mature Ruby-based DSL. Ecosystem: Strong community support with a large collection of \"cookbooks\" for common configurations. Enterprise Features: Provides a comprehensive suite of tools for enterprise users, including automated testing.","title":"Pros"},{"location":"DevOps/#cons_2","text":"Complexity: Steeper learning curve due to Ruby DSL and the complexity of managing cookbooks. Bootstrapping: Requires an agent to be installed on managed nodes, which can add to the setup time. Management: Managing a large number of cookbooks and dependencies can become challenging. Resources: Chef Official Documentation","title":"Cons"},{"location":"GCP/","text":"GCP Labs FIXME","title":"GCP Labs"},{"location":"GCP/#gcp-labs","text":"FIXME","title":"GCP Labs"},{"location":"Programming/","text":"Programming Go Examples https://github.com/trek10inc/awsets Node Python Falcon | The minimal, fast, and secure web framework for Python Falcon API Framework on Docker Typescript","title":"Programming"},{"location":"Programming/#programming","text":"","title":"Programming"},{"location":"Programming/#go","text":"","title":"Go"},{"location":"Programming/#examples","text":"https://github.com/trek10inc/awsets","title":"Examples"},{"location":"Programming/#node","text":"","title":"Node"},{"location":"Programming/#python","text":"Falcon | The minimal, fast, and secure web framework for Python Falcon API Framework on Docker","title":"Python"},{"location":"Programming/#typescript","text":"","title":"Typescript"},{"location":"Resources/","text":"Resources Icons / Logos Azure https://www.pngfind.com/pngs/m/597-5975946_microsoft-azure-logo-svg-hd-png-download.png GitHub https://cdn-icons-png.flaticon.com/512/25/25231.png Rackspace https://cdn.freebiesupply.com/logos/large/2x/rackspace-logo-svg-vector.svg Skill Icons https://github.com/tandpfun/skill-icons Social Logos https://github.com/Automattic/social-logos SVG Logos https://github.com/gilbarbara/logos Awesome Lists Awesome of Awesomes https://github.com/AdyKalra/awesome-of-awesomes AWS Security - https://github.com/jassics/awesome-aws-security AWS Self-hosted - https://github.com/fffaraz/awesome-selfhosted-aws AWS - https://github.com/donnemartin/awesome-aws GCP Certs https://github.com/sathishvj/awesome-gcp-certifications Security Hardening - https://github.com/decalage2/awesome-security-hardening","title":"Resources"},{"location":"Resources/#resources","text":"","title":"Resources"},{"location":"Resources/#icons-logos","text":"Azure https://www.pngfind.com/pngs/m/597-5975946_microsoft-azure-logo-svg-hd-png-download.png GitHub https://cdn-icons-png.flaticon.com/512/25/25231.png Rackspace https://cdn.freebiesupply.com/logos/large/2x/rackspace-logo-svg-vector.svg Skill Icons https://github.com/tandpfun/skill-icons Social Logos https://github.com/Automattic/social-logos SVG Logos https://github.com/gilbarbara/logos","title":"Icons / Logos"},{"location":"Resources/#awesome-lists","text":"Awesome of Awesomes https://github.com/AdyKalra/awesome-of-awesomes AWS Security - https://github.com/jassics/awesome-aws-security AWS Self-hosted - https://github.com/fffaraz/awesome-selfhosted-aws AWS - https://github.com/donnemartin/awesome-aws GCP Certs https://github.com/sathishvj/awesome-gcp-certifications Security Hardening - https://github.com/decalage2/awesome-security-hardening","title":"Awesome Lists"},{"location":"Security/","text":"Security Pages Red Team Sec DevSecOps https://github.com/OWASP/Top10 https://github.com/OWASP/vulnerable-container-hub https://github.com/OWASP/CheatSheetSeries Opensource OWASP Bug Logging Tool | OWASP Foundation https://github.com/cyberfascinate/CEHv11-Study-Material https://github.com/cyberfascinate/ISC2-CC-Study-Material Red Team https://github.com/cyberfascinate https://github.com/Ignitetechnologies/Mindmap https://github.com/ARPSyndicate/bug-bounty-recon-dataset","title":"Security"},{"location":"Security/#security","text":"","title":"Security"},{"location":"Security/#pages","text":"Red Team Sec","title":"Pages"},{"location":"Security/#devsecops","text":"https://github.com/OWASP/Top10 https://github.com/OWASP/vulnerable-container-hub https://github.com/OWASP/CheatSheetSeries","title":"DevSecOps"},{"location":"Security/#opensource","text":"OWASP Bug Logging Tool | OWASP Foundation https://github.com/cyberfascinate/CEHv11-Study-Material https://github.com/cyberfascinate/ISC2-CC-Study-Material","title":"Opensource"},{"location":"Security/#red-team","text":"https://github.com/cyberfascinate https://github.com/Ignitetechnologies/Mindmap https://github.com/ARPSyndicate/bug-bounty-recon-dataset","title":"Red Team"},{"location":"AWS/AWS100/","text":"AWS 100 Labs AWS 100 Labs Task 1: AWS Core Services Task 2: Infrastructure as Code with AWS CloudFormation Task 3: AWS Identity and Access Management (IAM) Task 4: AWS Networking and Security Task 5: AWS DevOps and CI/CD Task 6: AWS Monitoring and Optimization Task 1: AWS Core Services Title: Explore and Implement AWS Core Services Goal: Gain a foundational understanding of AWS core services, including EC2, S3, and VPC. Description: Set up an AWS account and familiarize yourself with the AWS Management Console. Launch an EC2 instance, create an S3 bucket for storage, and set up a basic VPC. Acceptance Criteria: Successfully navigate the AWS Management Console. EC2 instance is launched and accessible. S3 bucket is created and configured for basic file storage. VPC is set up with default settings and documented. Task 2: Infrastructure as Code with AWS CloudFormation Title: Deploy Infrastructure Using AWS CloudFormation Goal: Learn to define and deploy infrastructure as code on AWS using CloudFormation. Description: Write a CloudFormation template to deploy a simple web application stack, including EC2, Auto Scaling, and Elastic Load Balancer. Acceptance Criteria: CloudFormation template is written in YAML or JSON and stored in a version-controlled repository. Stack deployment is automated and repeatable using CloudFormation. Web application is accessible through the Elastic Load Balancer URL. Task 3: AWS Identity and Access Management (IAM) Title: Configure IAM for Secure Access Management Goal: Understand and implement secure access management using IAM. Description: Create IAM users, groups, and roles with specific policies to manage access to AWS resources. Acceptance Criteria: At least two IAM users and groups are created with appropriate permissions. IAM roles are utilized for EC2 instances to access other AWS services securely. Policies are documented and reviewed to ensure the principle of least privilege. Task 4: AWS Networking and Security Title: Design and Implement a Secure AWS Networking Solution Goal: Develop skills in designing, implementing, and managing secure and scalable AWS networking solutions. Description: Set up a more complex VPC with public and private subnets, NAT Gateway, and Route Tables. Implement Security Groups and Network ACLs for granular access control. Acceptance Criteria: VPC is configured with public and private subnets, NAT Gateway, and correct routing. Security Groups and Network ACLs are implemented with documented rules. Network connectivity is tested without compromising security. Task 5: AWS DevOps and CI/CD Title: Implement CI/CD Pipelines Using AWS CodePipeline and CodeBuild Goal: Gain hands-on experience with AWS DevOps tools to automate the build and deployment process. Description: Create a CI/CD pipeline using AWS CodePipeline, integrating with CodeBuild and CodeDeploy to automate the deployment of a web application. Acceptance Criteria: AWS CodePipeline is set up and integrated with a source control repository (e.g., GitHub). CodeBuild successfully compiles and tests the application code. CodeDeploy automates the deployment to EC2 instances or Elastic Beanstalk. The deployment process is documented, and the application is accessible post-deployment. Task 6: AWS Monitoring and Optimization Title: Implement Monitoring and Optimization with Amazon CloudWatch and AWS Trusted Advisor Goal: Learn to implement and configure monitoring and optimization features in AWS. Description: Configure Amazon CloudWatch for monitoring the web application's performance and setting up alarms. Use AWS Trusted Advisor to analyze and optimize the AWS environment for cost, performance, security, and fault tolerance. Acceptance Criteria: CloudWatch is configured with custom metrics and alarms for the web application. Key performance metrics are identified, monitored, and documented. Trusted Advisor recommendations are reviewed, implemented where appropriate, and documented.","title":"AWS 100 Labs"},{"location":"AWS/AWS100/#aws-100-labs","text":"AWS 100 Labs Task 1: AWS Core Services Task 2: Infrastructure as Code with AWS CloudFormation Task 3: AWS Identity and Access Management (IAM) Task 4: AWS Networking and Security Task 5: AWS DevOps and CI/CD Task 6: AWS Monitoring and Optimization","title":"AWS 100 Labs"},{"location":"AWS/AWS100/#task-1-aws-core-services","text":"Title: Explore and Implement AWS Core Services Goal: Gain a foundational understanding of AWS core services, including EC2, S3, and VPC. Description: Set up an AWS account and familiarize yourself with the AWS Management Console. Launch an EC2 instance, create an S3 bucket for storage, and set up a basic VPC. Acceptance Criteria: Successfully navigate the AWS Management Console. EC2 instance is launched and accessible. S3 bucket is created and configured for basic file storage. VPC is set up with default settings and documented.","title":"Task 1: AWS Core Services"},{"location":"AWS/AWS100/#task-2-infrastructure-as-code-with-aws-cloudformation","text":"Title: Deploy Infrastructure Using AWS CloudFormation Goal: Learn to define and deploy infrastructure as code on AWS using CloudFormation. Description: Write a CloudFormation template to deploy a simple web application stack, including EC2, Auto Scaling, and Elastic Load Balancer. Acceptance Criteria: CloudFormation template is written in YAML or JSON and stored in a version-controlled repository. Stack deployment is automated and repeatable using CloudFormation. Web application is accessible through the Elastic Load Balancer URL.","title":"Task 2: Infrastructure as Code with AWS CloudFormation"},{"location":"AWS/AWS100/#task-3-aws-identity-and-access-management-iam","text":"Title: Configure IAM for Secure Access Management Goal: Understand and implement secure access management using IAM. Description: Create IAM users, groups, and roles with specific policies to manage access to AWS resources. Acceptance Criteria: At least two IAM users and groups are created with appropriate permissions. IAM roles are utilized for EC2 instances to access other AWS services securely. Policies are documented and reviewed to ensure the principle of least privilege.","title":"Task 3: AWS Identity and Access Management (IAM)"},{"location":"AWS/AWS100/#task-4-aws-networking-and-security","text":"Title: Design and Implement a Secure AWS Networking Solution Goal: Develop skills in designing, implementing, and managing secure and scalable AWS networking solutions. Description: Set up a more complex VPC with public and private subnets, NAT Gateway, and Route Tables. Implement Security Groups and Network ACLs for granular access control. Acceptance Criteria: VPC is configured with public and private subnets, NAT Gateway, and correct routing. Security Groups and Network ACLs are implemented with documented rules. Network connectivity is tested without compromising security.","title":"Task 4: AWS Networking and Security"},{"location":"AWS/AWS100/#task-5-aws-devops-and-cicd","text":"Title: Implement CI/CD Pipelines Using AWS CodePipeline and CodeBuild Goal: Gain hands-on experience with AWS DevOps tools to automate the build and deployment process. Description: Create a CI/CD pipeline using AWS CodePipeline, integrating with CodeBuild and CodeDeploy to automate the deployment of a web application. Acceptance Criteria: AWS CodePipeline is set up and integrated with a source control repository (e.g., GitHub). CodeBuild successfully compiles and tests the application code. CodeDeploy automates the deployment to EC2 instances or Elastic Beanstalk. The deployment process is documented, and the application is accessible post-deployment.","title":"Task 5: AWS DevOps and CI/CD"},{"location":"AWS/AWS100/#task-6-aws-monitoring-and-optimization","text":"Title: Implement Monitoring and Optimization with Amazon CloudWatch and AWS Trusted Advisor Goal: Learn to implement and configure monitoring and optimization features in AWS. Description: Configure Amazon CloudWatch for monitoring the web application's performance and setting up alarms. Use AWS Trusted Advisor to analyze and optimize the AWS environment for cost, performance, security, and fault tolerance. Acceptance Criteria: CloudWatch is configured with custom metrics and alarms for the web application. Key performance metrics are identified, monitored, and documented. Trusted Advisor recommendations are reviewed, implemented where appropriate, and documented.","title":"Task 6: AWS Monitoring and Optimization"},{"location":"AWS/AWS200/","text":"AWS 200 Labs AWS 200 Labs AWS Services Launch an EC2 Instance Create a VPC Implement Autoscaling Implement RDS Implement S3 Implement CloudFormation Implement Lambda Implement CloudWatch Implement Route53 Implement IAM AWS DB Services Implement Aurora Database Implement DynamoDB Perform Regular Database Backups Implement CloudFront AWS Best Practices Perform Regular System Maintenance Implement GitOps Workflow Implement Continuous Integration Implement Automated Testing Implement Infrastructure as Code Implement Infrastructure Drift Detection AWS Cloud Native Implement Serverless Application Implement Chaos Engineering Monitoring Implement Infrastructure Monitoring Implement Distributed Tracing Implement A/B Testing AWS Services Launch an EC2 Instance Create an EC2 instance using Terraform. Use the latest Amazon Linux 2 AMI. Use a t2.micro instance type. Configure the instance to allow inbound traffic on port 22/SSH. Store the key pair used for SSH access in an S3 bucket with versioning enabled. Implement security best practices like limiting access and using secure passwords. Create a VPC Create a VPC using Terraform. Use the 10.0.0.0/16 IP range. Create two subnets: one public and one private. Configure the public subnet to have a route table that directs traffic to an Internet Gateway. Configure the private subnet to use a NAT Gateway for outbound traffic. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit. Implement Autoscaling Create an Auto Scaling Group using Terraform. Set up an ELB to distribute traffic across the instances. Configure the ASG to launch instances in the public subnet of the VPC created in Ticket 2. Set up scaling policies to automatically add or remove instances based on CPU usage. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit. Implement RDS Create an RDS instance using Terraform. Use the MySQL database engine. Use the db.t2.micro instance type. Configure the instance to allow inbound traffic on port 3306. Use an encrypted storage volume for the RDS instance. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement S3 Create an S3 bucket using Terraform. Enable versioning and encryption by default. Create a lifecycle rule that transitions objects to Glacier storage after 30 days. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement CloudFormation Create a CloudFormation stack using Terraform. Use the EC2 instance created in Ticket 1 as a resource in the stack. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Lambda Create a Lambda function using Terraform. Use the Python 3.8 runtime. Use an S3 bucket as a trigger for the Lambda function. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement CloudWatch Create a CloudWatch alarm using Terraform. Use the EC2 instance created in Ticket 1 as a metric for the alarm. Set up the alarm to notify an SNS topic when the CPU utilization exceeds 80% for more than 5 minutes. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Route53 Create a Route53 hosted zone using Terraform. Create an A record for the EC2 instance created in Ticket 1. Create an alias record for the ELB created in Ticket 3. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit. Implement IAM Create an IAM user using Terraform. Grant the user access to the S3 bucket created in Ticket 5. Assign the user the necessary permissions to list, read, and write objects in the bucket. Implement security best practices like using strong passwords, implementing least privilege, and enabling MFA for the user. AWS DB Services Implement Aurora Database Create an Aurora Database Cluster using Terraform. Use the MySQL-compatible database engine. Use the db.t2.small instance type. Configure the instance to allow inbound traffic on port 3306. Use an encrypted storagevolume for the Aurora cluster. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement DynamoDB Create a DynamoDB table using Terraform. Use a partition key and sort key to define the table's schema. Configure the table to use on-demand capacity. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Perform Regular Database Backups Create an AWS Backup plan using Terraform. Configure the plan to back up the Aurora Database created in Ticket 11 and the DynamoDB table created in Ticket 12. Set up a retention period of 30 days for backups. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement CloudFront Create a CloudFront distribution using Terraform. Configure the distribution to use the S3 bucket created in Ticket 5 as the origin. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. AWS Best Practices Perform Regular System Maintenance Create an AWS Systems Manager Maintenance Window using Terraform. Configure the maintenance window to run once a month. Implement a maintenance plan that includes OS and application patching, disk cleanup, and other regular maintenance tasks. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement GitOps Workflow Set up a Git repository using Terraform. Create a Kubernetes cluster using Terraform. Configure the cluster to use the Git repository as the source of truth for deployments. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Continuous Integration Set up a CI/CD pipeline using AWS CodePipeline and Terraform. Configure the pipeline to automatically build and test code changes on every commit to the Git repository created in Ticket 16. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Automated Testing Set up an automated testing framework using Terraform. Write unit tests for the application code and integration tests for the infrastructure code. Integrate the testing framework with the CI/CD pipeline created in Ticket 17. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Infrastructure as Code Refactor the Terraform code used in the previous tickets to use modules and reusable code. Implement a Git-based version control system for the Terraform code. Implement semantic versioning for the Terraform code. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Infrastructure Drift Detection Set up an infrastructure drift detection system using AWS Config and Terraform. Configure the system to detect changes to the infrastructure that are not reflected in the Terraform code. Integrate the drift detection system with the CI/CD pipeline created in Ticket 17. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. AWS Cloud Native Implement Serverless Application Develop a serverless application using AWS Lambda and Terraform. Use the Python 3.8 runtime. Use an S3 bucket as a trigger for the Lambda function. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Chaos Engineering Set up a chaos engineering framework using AWS Gremlin and Terraform. Use Gremlin to simulate various failure scenarios in the infrastructure and application components. Analyze the results of the chaos experiments and use them to improve the resilience of the system. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Monitoring Implement Infrastructure Monitoring Set up an infrastructure monitoring system using AWS CloudWatch and Terraform. Configure the system to monitor the application code and infrastructure metrics. Set up alarms to notify on-call engineers when critical metrics exceed predefined thresholds. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement Distributed Tracing Set up a distributed tracing system using AWS X-Ray and Terraform. Configure the system to trace requests across the application components. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit. Implement A/B Testing Set up an A/B testing system using AWS Lambda and Terraform. Configure the system to redirect a percentage of traffic to a different version of the application code. Use CloudWatch metrics and distributed tracing to track the results of the A/B testing. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"AWS 200 Labs"},{"location":"AWS/AWS200/#aws-200-labs","text":"AWS 200 Labs AWS Services Launch an EC2 Instance Create a VPC Implement Autoscaling Implement RDS Implement S3 Implement CloudFormation Implement Lambda Implement CloudWatch Implement Route53 Implement IAM AWS DB Services Implement Aurora Database Implement DynamoDB Perform Regular Database Backups Implement CloudFront AWS Best Practices Perform Regular System Maintenance Implement GitOps Workflow Implement Continuous Integration Implement Automated Testing Implement Infrastructure as Code Implement Infrastructure Drift Detection AWS Cloud Native Implement Serverless Application Implement Chaos Engineering Monitoring Implement Infrastructure Monitoring Implement Distributed Tracing Implement A/B Testing","title":"AWS 200 Labs"},{"location":"AWS/AWS200/#aws-services","text":"","title":"AWS Services"},{"location":"AWS/AWS200/#launch-an-ec2-instance","text":"Create an EC2 instance using Terraform. Use the latest Amazon Linux 2 AMI. Use a t2.micro instance type. Configure the instance to allow inbound traffic on port 22/SSH. Store the key pair used for SSH access in an S3 bucket with versioning enabled. Implement security best practices like limiting access and using secure passwords.","title":"Launch an EC2 Instance"},{"location":"AWS/AWS200/#create-a-vpc","text":"Create a VPC using Terraform. Use the 10.0.0.0/16 IP range. Create two subnets: one public and one private. Configure the public subnet to have a route table that directs traffic to an Internet Gateway. Configure the private subnet to use a NAT Gateway for outbound traffic. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit.","title":"Create a VPC"},{"location":"AWS/AWS200/#implement-autoscaling","text":"Create an Auto Scaling Group using Terraform. Set up an ELB to distribute traffic across the instances. Configure the ASG to launch instances in the public subnet of the VPC created in Ticket 2. Set up scaling policies to automatically add or remove instances based on CPU usage. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit.","title":"Implement Autoscaling"},{"location":"AWS/AWS200/#implement-rds","text":"Create an RDS instance using Terraform. Use the MySQL database engine. Use the db.t2.micro instance type. Configure the instance to allow inbound traffic on port 3306. Use an encrypted storage volume for the RDS instance. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement RDS"},{"location":"AWS/AWS200/#implement-s3","text":"Create an S3 bucket using Terraform. Enable versioning and encryption by default. Create a lifecycle rule that transitions objects to Glacier storage after 30 days. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement S3"},{"location":"AWS/AWS200/#implement-cloudformation","text":"Create a CloudFormation stack using Terraform. Use the EC2 instance created in Ticket 1 as a resource in the stack. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement CloudFormation"},{"location":"AWS/AWS200/#implement-lambda","text":"Create a Lambda function using Terraform. Use the Python 3.8 runtime. Use an S3 bucket as a trigger for the Lambda function. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Lambda"},{"location":"AWS/AWS200/#implement-cloudwatch","text":"Create a CloudWatch alarm using Terraform. Use the EC2 instance created in Ticket 1 as a metric for the alarm. Set up the alarm to notify an SNS topic when the CPU utilization exceeds 80% for more than 5 minutes. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement CloudWatch"},{"location":"AWS/AWS200/#implement-route53","text":"Create a Route53 hosted zone using Terraform. Create an A record for the EC2 instance created in Ticket 1. Create an alias record for the ELB created in Ticket 3. Implement security best practices like limiting access, using secure passwords, and encrypting data in transit.","title":"Implement Route53"},{"location":"AWS/AWS200/#implement-iam","text":"Create an IAM user using Terraform. Grant the user access to the S3 bucket created in Ticket 5. Assign the user the necessary permissions to list, read, and write objects in the bucket. Implement security best practices like using strong passwords, implementing least privilege, and enabling MFA for the user.","title":"Implement IAM"},{"location":"AWS/AWS200/#aws-db-services","text":"","title":"AWS DB Services"},{"location":"AWS/AWS200/#implement-aurora-database","text":"Create an Aurora Database Cluster using Terraform. Use the MySQL-compatible database engine. Use the db.t2.small instance type. Configure the instance to allow inbound traffic on port 3306. Use an encrypted storagevolume for the Aurora cluster. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Aurora Database"},{"location":"AWS/AWS200/#implement-dynamodb","text":"Create a DynamoDB table using Terraform. Use a partition key and sort key to define the table's schema. Configure the table to use on-demand capacity. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement DynamoDB"},{"location":"AWS/AWS200/#perform-regular-database-backups","text":"Create an AWS Backup plan using Terraform. Configure the plan to back up the Aurora Database created in Ticket 11 and the DynamoDB table created in Ticket 12. Set up a retention period of 30 days for backups. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Perform Regular Database Backups"},{"location":"AWS/AWS200/#implement-cloudfront","text":"Create a CloudFront distribution using Terraform. Configure the distribution to use the S3 bucket created in Ticket 5 as the origin. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement CloudFront"},{"location":"AWS/AWS200/#aws-best-practices","text":"","title":"AWS Best Practices"},{"location":"AWS/AWS200/#perform-regular-system-maintenance","text":"Create an AWS Systems Manager Maintenance Window using Terraform. Configure the maintenance window to run once a month. Implement a maintenance plan that includes OS and application patching, disk cleanup, and other regular maintenance tasks. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Perform Regular System Maintenance"},{"location":"AWS/AWS200/#implement-gitops-workflow","text":"Set up a Git repository using Terraform. Create a Kubernetes cluster using Terraform. Configure the cluster to use the Git repository as the source of truth for deployments. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement GitOps Workflow"},{"location":"AWS/AWS200/#implement-continuous-integration","text":"Set up a CI/CD pipeline using AWS CodePipeline and Terraform. Configure the pipeline to automatically build and test code changes on every commit to the Git repository created in Ticket 16. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Continuous Integration"},{"location":"AWS/AWS200/#implement-automated-testing","text":"Set up an automated testing framework using Terraform. Write unit tests for the application code and integration tests for the infrastructure code. Integrate the testing framework with the CI/CD pipeline created in Ticket 17. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Automated Testing"},{"location":"AWS/AWS200/#implement-infrastructure-as-code","text":"Refactor the Terraform code used in the previous tickets to use modules and reusable code. Implement a Git-based version control system for the Terraform code. Implement semantic versioning for the Terraform code. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Infrastructure as Code"},{"location":"AWS/AWS200/#implement-infrastructure-drift-detection","text":"Set up an infrastructure drift detection system using AWS Config and Terraform. Configure the system to detect changes to the infrastructure that are not reflected in the Terraform code. Integrate the drift detection system with the CI/CD pipeline created in Ticket 17. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Infrastructure Drift Detection"},{"location":"AWS/AWS200/#aws-cloud-native","text":"","title":"AWS Cloud Native"},{"location":"AWS/AWS200/#implement-serverless-application","text":"Develop a serverless application using AWS Lambda and Terraform. Use the Python 3.8 runtime. Use an S3 bucket as a trigger for the Lambda function. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Serverless Application"},{"location":"AWS/AWS200/#implement-chaos-engineering","text":"Set up a chaos engineering framework using AWS Gremlin and Terraform. Use Gremlin to simulate various failure scenarios in the infrastructure and application components. Analyze the results of the chaos experiments and use them to improve the resilience of the system. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Chaos Engineering"},{"location":"AWS/AWS200/#monitoring","text":"","title":"Monitoring"},{"location":"AWS/AWS200/#implement-infrastructure-monitoring","text":"Set up an infrastructure monitoring system using AWS CloudWatch and Terraform. Configure the system to monitor the application code and infrastructure metrics. Set up alarms to notify on-call engineers when critical metrics exceed predefined thresholds. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Infrastructure Monitoring"},{"location":"AWS/AWS200/#implement-distributed-tracing","text":"Set up a distributed tracing system using AWS X-Ray and Terraform. Configure the system to trace requests across the application components. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement Distributed Tracing"},{"location":"AWS/AWS200/#implement-ab-testing","text":"Set up an A/B testing system using AWS Lambda and Terraform. Configure the system to redirect a percentage of traffic to a different version of the application code. Use CloudWatch metrics and distributed tracing to track the results of the A/B testing. Implement security best practices like limiting access, using secure passwords, and encrypting data at rest and in transit.","title":"Implement A/B Testing"},{"location":"AWS/ec2-userdata/","text":"Lab: Using AWS EC2 UserData Using AWS EC2 UserData to Customize an Autoscaling Group Instance Overview In this lab, you will learn how to use AWS EC2 UserData to customize an autoscaling group instance. This lab assumes that you already have a basic understanding of the AWS EC2 service and the Autoscaling Group service. Prerequisites An AWS account with access to EC2 and Autoscaling Group services A basic understanding of the AWS EC2 service and the Autoscaling Group service Familiarity with scripting languages such as Bash or Python Familiarity with Terraform or Serverless Framework Objectives By the end of this lab, you will be able to: Understand what UserData is and how it can be used to customize an autoscaling group instance Create a script using UserData that will run when an autoscaling group instance is launched Task 1: Understanding UserData UserData is a feature of Amazon EC2 that allows you to pass data into your instances when they are launched. This data can be used to configure and customize your instances. In this lab, we will use UserData to customize an autoscaling group instance. Task 2: Creating a Script Using UserData In this task, you will create a script using UserData that will run when an autoscaling group instance is launched. The script should perform any necessary configuration tasks such as installing packages, setting environment variables, etc. The script should be written in either Bash or Python and should be saved in a file named userdata.sh or userdata.py . Task 3: Configuring the Autoscaling Group Instance Once you have created your script, you need to configure your autoscaling group instance so that it runs the script when it is launched. To do this, open the Amazon EC2 console and select your autoscaling group from the list of instances. Then select \u201cEdit\u201d from the Actions menu and enter your userdata script in the \u201cUser Data\u201d field. Finally, click \u201cSave\u201d to save your changes. Conclusion In this lab, you learned how to use AWS EC2 UserData to customize an autoscaling group instance by creating a script using either Bash or Python and configuring it in the Amazon EC2 console. You should now have a better understanding of how userdata works and how it can be used for customizing instances in an autoscaling group.","title":"Lab: Using AWS EC2 UserData"},{"location":"AWS/ec2-userdata/#lab-using-aws-ec2-userdata","text":"Using AWS EC2 UserData to Customize an Autoscaling Group Instance","title":"Lab: Using AWS EC2 UserData"},{"location":"AWS/ec2-userdata/#overview","text":"In this lab, you will learn how to use AWS EC2 UserData to customize an autoscaling group instance. This lab assumes that you already have a basic understanding of the AWS EC2 service and the Autoscaling Group service.","title":"Overview"},{"location":"AWS/ec2-userdata/#prerequisites","text":"An AWS account with access to EC2 and Autoscaling Group services A basic understanding of the AWS EC2 service and the Autoscaling Group service Familiarity with scripting languages such as Bash or Python Familiarity with Terraform or Serverless Framework","title":"Prerequisites"},{"location":"AWS/ec2-userdata/#objectives","text":"By the end of this lab, you will be able to: Understand what UserData is and how it can be used to customize an autoscaling group instance Create a script using UserData that will run when an autoscaling group instance is launched","title":"Objectives"},{"location":"AWS/ec2-userdata/#task-1-understanding-userdata","text":"UserData is a feature of Amazon EC2 that allows you to pass data into your instances when they are launched. This data can be used to configure and customize your instances. In this lab, we will use UserData to customize an autoscaling group instance.","title":"Task 1: Understanding UserData"},{"location":"AWS/ec2-userdata/#task-2-creating-a-script-using-userdata","text":"In this task, you will create a script using UserData that will run when an autoscaling group instance is launched. The script should perform any necessary configuration tasks such as installing packages, setting environment variables, etc. The script should be written in either Bash or Python and should be saved in a file named userdata.sh or userdata.py .","title":"Task 2: Creating a Script Using UserData"},{"location":"AWS/ec2-userdata/#task-3-configuring-the-autoscaling-group-instance","text":"Once you have created your script, you need to configure your autoscaling group instance so that it runs the script when it is launched. To do this, open the Amazon EC2 console and select your autoscaling group from the list of instances. Then select \u201cEdit\u201d from the Actions menu and enter your userdata script in the \u201cUser Data\u201d field. Finally, click \u201cSave\u201d to save your changes.","title":"Task 3: Configuring the Autoscaling Group Instance"},{"location":"AWS/ec2-userdata/#conclusion","text":"In this lab, you learned how to use AWS EC2 UserData to customize an autoscaling group instance by creating a script using either Bash or Python and configuring it in the Amazon EC2 console. You should now have a better understanding of how userdata works and how it can be used for customizing instances in an autoscaling group.","title":"Conclusion"},{"location":"Azure/Azure100/","text":"Azure 100 Labs Azure 100 Labs Task 1: Azure Fundamentals Task 2: Infrastructure as Code with Azure ARM Templates Task 3: Azure Active Directory and RBAC Task 4: Implement and Manage Azure Networking Task 5: Azure DevOps and CI/CD Pipelines Task 6: Azure Monitoring and Diagnostics Task 1: Azure Fundamentals Understand and Implement Azure Fundamentals Goal: Gain a foundational understanding of Azure services, management tools, and security features. Description: Explore core Azure services, including Azure Compute, Storage, and Networking services. Set up a basic Azure environment with a virtual network and a couple of VMs. Acceptance Criteria Successfully create an Azure account and navigate the Azure portal. Deploy a virtual network and two VMs within the Azure portal. Document the process and key learnings in a shared repository. Task 2: Infrastructure as Code with Azure ARM Templates Deploy Infrastructure Using Azure Resource Manager (ARM) Templates Goal: Learn how to define and deploy infrastructure as code on Azure using ARM templates. Description: Create an ARM template to deploy a scalable web application infrastructure. Include resources such as Azure App Service, SQL Database, and Application Insights. Acceptance Criteria ARM template is written in JSON and stored in a version-controlled repository. Infrastructure deployment is automated and repeatable with the ARM template. Validate the deployment through the Azure portal and ensure all components are correctly configured. Task 3: Azure Active Directory and RBAC Configure Azure Active Directory (AD) and Role-Based Access Control (RBAC) Goal: Understand and implement security and identity management through Azure AD and RBAC. Description: Set up an Azure AD instance and create user groups. Assign RBAC roles to different user groups to manage access to Azure resources. Acceptance Criteria The Azure AD instance is correctly set up with at least two user groups. RBAC roles are assigned to these groups, ensuring the principle of least privilege. Successfully demonstrate access control by having different users access resources based on their RBAC roles. Task 4: Implement and Manage Azure Networking Design and Implement a Secure Azure Networking Solution Goal: Develop skills in designing, implementing, and managing secure and scalable Azure networking solutions. Description: Create a Virtual Network with subnetting to separate different parts of the system. Implement Network Security Groups (NSGs) and Application Security Groups (ASGs) for fine-grained access control. Acceptance Criteria Virtual Network and subnets are correctly configured and documented. NSGs and ASGs are implemented to secure network access, with rules documented. Connectivity is tested and verified between different parts of the system without compromising security. Task 5: Azure DevOps and CI/CD Pipelines Build CI/CD Pipelines Using Azure DevOps Goal Gain hands-on experience with Azure DevOps Services to automate the build and deployment process. Description: Set up an Azure DevOps project and connect it to a source control repository. Create CI/CD pipelines to automate the build and deployment of a web application to Azure App Service. Acceptance Criteria Azure DevOps project is correctly set up and integrated with a version control system. CI pipeline successfully builds the code upon each commit. CD pipeline deploys the build artifacts to Azure App Service automatically upon successful build. Deployment is verified by accessing the deployed application. Task 6: Azure Monitoring and Diagnostics Implement Monitoring, Logging, and Diagnostics in Azure Goal: Learn to implement and configure monitoring and diagnostics features in Azure to maintain and optimize applications. Description: Configure Azure Monitor and Application Insights for a web application. Set up alerts and diagnostic logging to track performance issues and exceptions. Acceptance Criteria Azure Monitor and Application Insights are correctly configured for the web application. Custom alerts are set up for key metrics and log searches. Successfully demonstrate the ability to diagnose and troubleshoot an issue using the logs and metrics collected.","title":"Azure 100 Labs"},{"location":"Azure/Azure100/#azure-100-labs","text":"Azure 100 Labs Task 1: Azure Fundamentals Task 2: Infrastructure as Code with Azure ARM Templates Task 3: Azure Active Directory and RBAC Task 4: Implement and Manage Azure Networking Task 5: Azure DevOps and CI/CD Pipelines Task 6: Azure Monitoring and Diagnostics","title":"Azure 100 Labs"},{"location":"Azure/Azure100/#task-1-azure-fundamentals","text":"Understand and Implement Azure Fundamentals Goal: Gain a foundational understanding of Azure services, management tools, and security features. Description: Explore core Azure services, including Azure Compute, Storage, and Networking services. Set up a basic Azure environment with a virtual network and a couple of VMs. Acceptance Criteria Successfully create an Azure account and navigate the Azure portal. Deploy a virtual network and two VMs within the Azure portal. Document the process and key learnings in a shared repository.","title":"Task 1: Azure Fundamentals"},{"location":"Azure/Azure100/#task-2-infrastructure-as-code-with-azure-arm-templates","text":"Deploy Infrastructure Using Azure Resource Manager (ARM) Templates Goal: Learn how to define and deploy infrastructure as code on Azure using ARM templates. Description: Create an ARM template to deploy a scalable web application infrastructure. Include resources such as Azure App Service, SQL Database, and Application Insights. Acceptance Criteria ARM template is written in JSON and stored in a version-controlled repository. Infrastructure deployment is automated and repeatable with the ARM template. Validate the deployment through the Azure portal and ensure all components are correctly configured.","title":"Task 2: Infrastructure as Code with Azure ARM Templates"},{"location":"Azure/Azure100/#task-3-azure-active-directory-and-rbac","text":"Configure Azure Active Directory (AD) and Role-Based Access Control (RBAC) Goal: Understand and implement security and identity management through Azure AD and RBAC. Description: Set up an Azure AD instance and create user groups. Assign RBAC roles to different user groups to manage access to Azure resources. Acceptance Criteria The Azure AD instance is correctly set up with at least two user groups. RBAC roles are assigned to these groups, ensuring the principle of least privilege. Successfully demonstrate access control by having different users access resources based on their RBAC roles.","title":"Task 3: Azure Active Directory and RBAC"},{"location":"Azure/Azure100/#task-4-implement-and-manage-azure-networking","text":"Design and Implement a Secure Azure Networking Solution Goal: Develop skills in designing, implementing, and managing secure and scalable Azure networking solutions. Description: Create a Virtual Network with subnetting to separate different parts of the system. Implement Network Security Groups (NSGs) and Application Security Groups (ASGs) for fine-grained access control. Acceptance Criteria Virtual Network and subnets are correctly configured and documented. NSGs and ASGs are implemented to secure network access, with rules documented. Connectivity is tested and verified between different parts of the system without compromising security.","title":"Task 4: Implement and Manage Azure Networking"},{"location":"Azure/Azure100/#task-5-azure-devops-and-cicd-pipelines","text":"Build CI/CD Pipelines Using Azure DevOps Goal Gain hands-on experience with Azure DevOps Services to automate the build and deployment process. Description: Set up an Azure DevOps project and connect it to a source control repository. Create CI/CD pipelines to automate the build and deployment of a web application to Azure App Service. Acceptance Criteria Azure DevOps project is correctly set up and integrated with a version control system. CI pipeline successfully builds the code upon each commit. CD pipeline deploys the build artifacts to Azure App Service automatically upon successful build. Deployment is verified by accessing the deployed application.","title":"Task 5: Azure DevOps and CI/CD Pipelines"},{"location":"Azure/Azure100/#task-6-azure-monitoring-and-diagnostics","text":"Implement Monitoring, Logging, and Diagnostics in Azure Goal: Learn to implement and configure monitoring and diagnostics features in Azure to maintain and optimize applications. Description: Configure Azure Monitor and Application Insights for a web application. Set up alerts and diagnostic logging to track performance issues and exceptions. Acceptance Criteria Azure Monitor and Application Insights are correctly configured for the web application. Custom alerts are set up for key metrics and log searches. Successfully demonstrate the ability to diagnose and troubleshoot an issue using the logs and metrics collected.","title":"Task 6: Azure Monitoring and Diagnostics"},{"location":"Azure/Azure200/","text":"Azure 200 Labs Azure 200 Labs Create a basic Azure virtual machine Create an Azure storage account Create a Terraform script to deploy a web application Set up a database in Azure Schedule regular maintenance tasks on databases in Azure Implement semantic versioning of infrastructure code Set up continuous integration/continuous delivery (CI/CD) pipelines Implement GitOps workflows Develop scripts to automate deployment processes Create a basic Azure virtual machine Create an Azure virtual machine with the appropriate size and configuration. Ensure that the VM is secure and follows best practices for cloud security. Create an Azure storage account Create an Azure storage account with the appropriate configuration. Ensure that the storage account is secure and follows best practices for cloud security. Create a Terraform script to deploy a web application Create a Terraform script to deploy a web application to an Azure virtual machine. Ensure that the script follows best practices for Terraform scripting and is secure. Set up a database in Azure Set up a database in Azure with the appropriate configuration. Ensure that the database is secure and follows best practices for cloud security. Perform regular backups of databases in Azure Perform regular backups of databases in Azure using the appropriate tools and techniques. Ensure that the backups are secure and follow best practices for cloud security. Schedule regular maintenance tasks on databases in Azure Schedule regular maintenance tasks on databases in Azure using the appropriate tools and techniques. Ensure that all maintenance tasks are completed successfully and follow best practices for cloud security. Implement semantic versioning of infrastructure code Implement semantic versioning of infrastructure code using the appropriate tools and techniques. Ensure that all changes are tracked properly and follow best practices for version control systems. Set up continuous integration/continuous delivery (CI/CD) pipelines Set up continuous integration/continuous delivery (CI/CD) pipelines using the appropriate tools and techniques. Ensure that all pipelines are secure and follow best practices for CI/CD pipelines in the cloud. Implement GitOps workflows Implement GitOps workflows using the appropriate tools and techniques. Ensure that all GitOps workflows are secure and follow best practices for GitOps workflows in the cloud. Develop scripts to automate deployment processes Develop scripts to automate deployment processes using the appropriate programming language(s). Ensure that all scripts are secure and follow best practices for scripting languages used in DevOps automation processes in the cloud.","title":"Azure 200 Labs"},{"location":"Azure/Azure200/#azure-200-labs","text":"Azure 200 Labs Create a basic Azure virtual machine Create an Azure storage account Create a Terraform script to deploy a web application Set up a database in Azure Schedule regular maintenance tasks on databases in Azure Implement semantic versioning of infrastructure code Set up continuous integration/continuous delivery (CI/CD) pipelines Implement GitOps workflows Develop scripts to automate deployment processes","title":"Azure 200 Labs"},{"location":"Azure/Azure200/#create-a-basic-azure-virtual-machine","text":"Create an Azure virtual machine with the appropriate size and configuration. Ensure that the VM is secure and follows best practices for cloud security.","title":"Create a basic Azure virtual machine"},{"location":"Azure/Azure200/#create-an-azure-storage-account","text":"Create an Azure storage account with the appropriate configuration. Ensure that the storage account is secure and follows best practices for cloud security.","title":"Create an Azure storage account"},{"location":"Azure/Azure200/#create-a-terraform-script-to-deploy-a-web-application","text":"Create a Terraform script to deploy a web application to an Azure virtual machine. Ensure that the script follows best practices for Terraform scripting and is secure.","title":"Create a Terraform script to deploy a web application"},{"location":"Azure/Azure200/#set-up-a-database-in-azure","text":"Set up a database in Azure with the appropriate configuration. Ensure that the database is secure and follows best practices for cloud security. Perform regular backups of databases in Azure Perform regular backups of databases in Azure using the appropriate tools and techniques. Ensure that the backups are secure and follow best practices for cloud security.","title":"Set up a database in Azure"},{"location":"Azure/Azure200/#schedule-regular-maintenance-tasks-on-databases-in-azure","text":"Schedule regular maintenance tasks on databases in Azure using the appropriate tools and techniques. Ensure that all maintenance tasks are completed successfully and follow best practices for cloud security.","title":"Schedule regular maintenance tasks on databases in Azure"},{"location":"Azure/Azure200/#implement-semantic-versioning-of-infrastructure-code","text":"Implement semantic versioning of infrastructure code using the appropriate tools and techniques. Ensure that all changes are tracked properly and follow best practices for version control systems.","title":"Implement semantic versioning of infrastructure code"},{"location":"Azure/Azure200/#set-up-continuous-integrationcontinuous-delivery-cicd-pipelines","text":"Set up continuous integration/continuous delivery (CI/CD) pipelines using the appropriate tools and techniques. Ensure that all pipelines are secure and follow best practices for CI/CD pipelines in the cloud.","title":"Set up continuous integration/continuous delivery (CI/CD) pipelines"},{"location":"Azure/Azure200/#implement-gitops-workflows","text":"Implement GitOps workflows using the appropriate tools and techniques. Ensure that all GitOps workflows are secure and follow best practices for GitOps workflows in the cloud.","title":"Implement GitOps workflows"},{"location":"Azure/Azure200/#develop-scripts-to-automate-deployment-processes","text":"Develop scripts to automate deployment processes using the appropriate programming language(s). Ensure that all scripts are secure and follow best practices for scripting languages used in DevOps automation processes in the cloud.","title":"Develop scripts to automate deployment processes"},{"location":"Azure/AzureMonitoring/","text":"Azure Monitoring The heart of Azure monitoring is KQL. Check out some resources below for examples and snippets. If someting is missing please feel free to open a PR. On those repos not just here :) Resources https://github.com/microsoft/AzureMonitorCommunity https://github.com/reprise99/Sentinel-Queries","title":"Azure Monitoring"},{"location":"Azure/AzureMonitoring/#azure-monitoring","text":"The heart of Azure monitoring is KQL. Check out some resources below for examples and snippets. If someting is missing please feel free to open a PR. On those repos not just here :)","title":"Azure Monitoring"},{"location":"Azure/AzureMonitoring/#resources","text":"https://github.com/microsoft/AzureMonitorCommunity https://github.com/reprise99/Sentinel-Queries","title":"Resources"},{"location":"GCP/gcp100/","text":"GCP Labs GCP Labs Create a GCP Compute Engine instance Set up a GCP Cloud Storage Bucket Create a GCP Kubernetes Cluster Set up a GCP Cloud SQL Instance Configure IAM roles for users Set up monitoring and alerting Deploy an application on App Engine Configure Networking Resources Backup data stored in Cloud Storage Buckets Migrate existing applications to GCP Create a GCP Compute Engine instance Create a GCP Compute Engine instance with the appropriate machine type and storage size for the application. Use Terraform to create the instance and ensure that best practices such as setting up firewall rules and configuring logging are implemented. Set up a GCP Cloud Storage Bucket Set up a GCP Cloud Storage Bucket using Terraform. Ensure that the bucket is configured with appropriate access control settings and logging is enabled. Create a GCP Kubernetes Cluster Create a GCP Kubernetes Cluster using Terraform. Ensure that the cluster is configured with appropriate access control settings and logging is enabled. Set up a GCP Cloud SQL Instance Set up a GCP Cloud SQL Instance using Terraform. Ensure that the instance is configured with appropriate access control settings and logging is enabled. Configure IAM roles for users Configure IAM roles for users using Terraform to grant them appropriate access to resources in the cloud environment. Set up monitoring and alerting Set up monitoring and alerting using Terraform to ensure that any issues or changes in the environment are detected quickly. Deploy an application on App Engine Deploy an application on App Engine using Terraform and ensure that best practices such as configuring logging are implemented. Configure Networking Resources Configure Networking Resources such as VPCs, subnets, routes etc using Terraform to ensure proper segmentation of resources in the cloud environment. Backup data stored in Cloud Storage Buckets Backup data stored in Cloud Storage Buckets using Terraform to ensure data integrity in case of any unexpected events or disasters. Migrate existing applications to GCP Migrate existing applications to GCP using Terraform and ensure that all necessary components such as networking resources are configured correctly for optimal performance of the application in the cloud environment. Tasks around databases: Set up replication for database instances; Monitor database performance; Automate backup of database instances; Create database users; Configure security policies for databases; Monitor database logs; Maintain database indices; Automate patching of databases; Perform regular maintenance tasks on databases; Configure high availability for databases. Tasks related to DevOps: Set up version control system (e.g., Git); Implement semantic versioning (e.g., SemVer); Set up continuous integration/continuous deployment (CI/CD) pipelines; Implement infrastructure as code (IaC) principles (e.g., use of terraform); Implement GitOps workflows (e.g., use of GitHub Actions); Configure automated testing frameworks (e.g., use of Jest); Create automated deployment scripts (e.g., use of Ansible); Monitor system performance metrics (e.g., use of Prometheus); Implement security best practices (e","title":"GCP Labs"},{"location":"GCP/gcp100/#gcp-labs","text":"GCP Labs Create a GCP Compute Engine instance Set up a GCP Cloud Storage Bucket Create a GCP Kubernetes Cluster Set up a GCP Cloud SQL Instance Configure IAM roles for users Set up monitoring and alerting Deploy an application on App Engine Configure Networking Resources Backup data stored in Cloud Storage Buckets Migrate existing applications to GCP","title":"GCP Labs"},{"location":"GCP/gcp100/#create-a-gcp-compute-engine-instance","text":"Create a GCP Compute Engine instance with the appropriate machine type and storage size for the application. Use Terraform to create the instance and ensure that best practices such as setting up firewall rules and configuring logging are implemented.","title":"Create a GCP Compute Engine instance"},{"location":"GCP/gcp100/#set-up-a-gcp-cloud-storage-bucket","text":"Set up a GCP Cloud Storage Bucket using Terraform. Ensure that the bucket is configured with appropriate access control settings and logging is enabled.","title":"Set up a GCP Cloud Storage Bucket"},{"location":"GCP/gcp100/#create-a-gcp-kubernetes-cluster","text":"Create a GCP Kubernetes Cluster using Terraform. Ensure that the cluster is configured with appropriate access control settings and logging is enabled.","title":"Create a GCP Kubernetes Cluster"},{"location":"GCP/gcp100/#set-up-a-gcp-cloud-sql-instance","text":"Set up a GCP Cloud SQL Instance using Terraform. Ensure that the instance is configured with appropriate access control settings and logging is enabled.","title":"Set up a GCP Cloud SQL Instance"},{"location":"GCP/gcp100/#configure-iam-roles-for-users","text":"Configure IAM roles for users using Terraform to grant them appropriate access to resources in the cloud environment.","title":"Configure IAM roles for users"},{"location":"GCP/gcp100/#set-up-monitoring-and-alerting","text":"Set up monitoring and alerting using Terraform to ensure that any issues or changes in the environment are detected quickly.","title":"Set up monitoring and alerting"},{"location":"GCP/gcp100/#deploy-an-application-on-app-engine","text":"Deploy an application on App Engine using Terraform and ensure that best practices such as configuring logging are implemented.","title":"Deploy an application on App Engine"},{"location":"GCP/gcp100/#configure-networking-resources","text":"Configure Networking Resources such as VPCs, subnets, routes etc using Terraform to ensure proper segmentation of resources in the cloud environment.","title":"Configure Networking Resources"},{"location":"GCP/gcp100/#backup-data-stored-in-cloud-storage-buckets","text":"Backup data stored in Cloud Storage Buckets using Terraform to ensure data integrity in case of any unexpected events or disasters.","title":"Backup data stored in Cloud Storage Buckets"},{"location":"GCP/gcp100/#migrate-existing-applications-to-gcp","text":"Migrate existing applications to GCP using Terraform and ensure that all necessary components such as networking resources are configured correctly for optimal performance of the application in the cloud environment. Tasks around databases: Set up replication for database instances; Monitor database performance; Automate backup of database instances; Create database users; Configure security policies for databases; Monitor database logs; Maintain database indices; Automate patching of databases; Perform regular maintenance tasks on databases; Configure high availability for databases. Tasks related to DevOps: Set up version control system (e.g., Git); Implement semantic versioning (e.g., SemVer); Set up continuous integration/continuous deployment (CI/CD) pipelines; Implement infrastructure as code (IaC) principles (e.g., use of terraform); Implement GitOps workflows (e.g., use of GitHub Actions); Configure automated testing frameworks (e.g., use of Jest); Create automated deployment scripts (e.g., use of Ansible); Monitor system performance metrics (e.g., use of Prometheus); Implement security best practices (e","title":"Migrate existing applications to GCP"},{"location":"consulting/migration-discovery/","text":"Lab: Discovery for a Migration Project from a Datacenter to the Cloud Lab: Discovery for a Migration Project from a Datacenter to the Cloud Overview Objectives Prerequisites Tasks Conclusion Overview This lab will teach you how to do discovery for a migration project from a data center to the cloud. You will learn about the different tools and techniques available for discovering your existing infrastructure and applications, as well as how to use them to plan and execute your migration project. Objectives At the end of this lab, you will be able to: Identify the different tools and techniques available for discovery in a migration project. Use Terraform and Serverless Framework to discover existing infrastructure and applications. Plan and execute your migration project using the information gathered during discovery. Prerequisites Before beginning this lab, you should have: A basic understanding of Terraform and Serverless Framework. Access to an existing data center or cloud environment. Tasks Identify Tools & Techniques - Identify the different tools and techniques available for discovering your existing infrastructure and applications in order to plan and execute your migration project. Some examples include using Terraform or Serverless Framework for infrastructure discovery or using application profiling tools such as AppDynamics or New Relic for application discovery. Use Terraform & Serverless Framework - Use Terraform and Serverless Framework to discover existing infrastructure components such as servers, databases, storage systems, networks, etc., as well as any applications running on those components. This can be done by writing code that queries APIs or by manually inspecting each component individually. Plan & Execute Migration - Once you have identified all of the components that need to be migrated, use this information to plan out your migration strategy. This should include details such as which components need to be migrated first, what order they should be migrated in, what resources are needed for each component, etc., so that you can ensure a successful migration with minimal disruption of services. Conclusion In this lab, you learned how to do discovery for a migration project from a data center to the cloud by identifying different tools and techniques available for discovering existing infrastructure and applications, using Terraform and Serverless Framework for discovery purposes, and planning out your migration strategy based on the information gathered during discovery. With this knowledge in hand, you are now ready to begin planning out your own cloud migrations!","title":"Lab: Discovery for a Migration Project from a Datacenter to the Cloud"},{"location":"consulting/migration-discovery/#lab-discovery-for-a-migration-project-from-a-datacenter-to-the-cloud","text":"Lab: Discovery for a Migration Project from a Datacenter to the Cloud Overview Objectives Prerequisites Tasks Conclusion","title":"Lab: Discovery for a Migration Project from a Datacenter to the Cloud"},{"location":"consulting/migration-discovery/#overview","text":"This lab will teach you how to do discovery for a migration project from a data center to the cloud. You will learn about the different tools and techniques available for discovering your existing infrastructure and applications, as well as how to use them to plan and execute your migration project.","title":"Overview"},{"location":"consulting/migration-discovery/#objectives","text":"At the end of this lab, you will be able to: Identify the different tools and techniques available for discovery in a migration project. Use Terraform and Serverless Framework to discover existing infrastructure and applications. Plan and execute your migration project using the information gathered during discovery.","title":"Objectives"},{"location":"consulting/migration-discovery/#prerequisites","text":"Before beginning this lab, you should have: A basic understanding of Terraform and Serverless Framework. Access to an existing data center or cloud environment.","title":"Prerequisites"},{"location":"consulting/migration-discovery/#tasks","text":"Identify Tools & Techniques - Identify the different tools and techniques available for discovering your existing infrastructure and applications in order to plan and execute your migration project. Some examples include using Terraform or Serverless Framework for infrastructure discovery or using application profiling tools such as AppDynamics or New Relic for application discovery. Use Terraform & Serverless Framework - Use Terraform and Serverless Framework to discover existing infrastructure components such as servers, databases, storage systems, networks, etc., as well as any applications running on those components. This can be done by writing code that queries APIs or by manually inspecting each component individually. Plan & Execute Migration - Once you have identified all of the components that need to be migrated, use this information to plan out your migration strategy. This should include details such as which components need to be migrated first, what order they should be migrated in, what resources are needed for each component, etc., so that you can ensure a successful migration with minimal disruption of services.","title":"Tasks"},{"location":"consulting/migration-discovery/#conclusion","text":"In this lab, you learned how to do discovery for a migration project from a data center to the cloud by identifying different tools and techniques available for discovering existing infrastructure and applications, using Terraform and Serverless Framework for discovery purposes, and planning out your migration strategy based on the information gathered during discovery. With this knowledge in hand, you are now ready to begin planning out your own cloud migrations!","title":"Conclusion"},{"location":"consulting/workload-discover/","text":"Lab: Workload Discovery in a Given Account Lab: Workload Discovery in a Given Account What is Workload Discovery? Questions to Consider When Doing Workload Discovery This lab will teach you how to do discovery for workloads in a given account. We will cover the following topics: What is workload discovery? Questions to consider when doing workload discovery How to use Terraform and Serverless Framework for workload discovery What is Workload Discovery? Workload discovery is the process of identifying and cataloging all the components of an application or system, including servers, networks, storage, applications, and services. The goal of this process is to gain an understanding of the environment so that it can be managed effectively. This includes understanding how the components interact with each other and how they are configured. Questions to Consider When Doing Workload Discovery When doing workload discovery, it's important to consider a few key questions: What types of resources are present in the account? Are there any security or compliance requirements that need to be met? Are there any existing policies or processes that need to be taken into account? Are there any dependencies between different resources or services? Are there any performance requirements that need to be met?","title":"Lab: Workload Discovery in a Given Account"},{"location":"consulting/workload-discover/#lab-workload-discovery-in-a-given-account","text":"Lab: Workload Discovery in a Given Account What is Workload Discovery? Questions to Consider When Doing Workload Discovery This lab will teach you how to do discovery for workloads in a given account. We will cover the following topics: What is workload discovery? Questions to consider when doing workload discovery How to use Terraform and Serverless Framework for workload discovery","title":"Lab: Workload Discovery in a Given Account"},{"location":"consulting/workload-discover/#what-is-workload-discovery","text":"Workload discovery is the process of identifying and cataloging all the components of an application or system, including servers, networks, storage, applications, and services. The goal of this process is to gain an understanding of the environment so that it can be managed effectively. This includes understanding how the components interact with each other and how they are configured.","title":"What is Workload Discovery?"},{"location":"consulting/workload-discover/#questions-to-consider-when-doing-workload-discovery","text":"When doing workload discovery, it's important to consider a few key questions: What types of resources are present in the account? Are there any security or compliance requirements that need to be met? Are there any existing policies or processes that need to be taken into account? Are there any dependencies between different resources or services? Are there any performance requirements that need to be met?","title":"Questions to Consider When Doing Workload Discovery"},{"location":"containers/docker-100/","text":"Introduction to Docker and Building Docker Images Introduction to Docker and Building Docker Images What is Docker? Benefits of Using Docker Prerequisites Exercise Conculsion Docker is a popular open-source platform for developing, shipping, and running applications. It provides an easy way to package applications into containers that can be run on any system with the Docker Engine installed. This lab will provide an introduction to Docker and guide you through the process of building your own Docker images. What is Docker? Docker is a containerization platform that allows you to package your application into a container image. A container image is a lightweight, stand-alone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Containers are isolated from each other and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines. Benefits of Using Docker Portability: You can easily move your application from one environment to another without having to worry about compatibility issues or dependencies. Scalability: You can quickly scale up or down your application depending on the load it needs to handle. Security: Containers provide an additional layer of security as they are isolated from each other and the host operating system. Prerequisites Before you begin this lab you should have some basic understanding of command line tools such as docker , docker-compose and docker-machine . Additionally, you should have access to a machine with the latest version of Docker installed on it. Exercise In this exercise, we will build our first docker image using the docker build command. We will use a simple NodeJS web server as our example application for this exercise: const http = require(\"http\"); const hostname = \"127.0.0.1\"; const port = 3000; const server = http.createServer((req, res) => { res.statusCode = 200; res.setHeader(\"Content-Type\", \"text/plain\"); res.end(\"Hello World\\n\"); }); server.listen(port, hostname, () => { console.log(`Server running at http://${hostname}:${port}/`); }); To build our image we will need two files - a Dockerfile which contains instructions for building our image and an index.js file which contains our NodeJS web server code (as shown above). Create these two files in the same directory on your Linux machine then open up the Dockerfile in your text editor of choice (e.g., vim). Add the following lines to it: FROM node:latest This line tells docker which base image we want to use for our own custom image - in this case we are using the latest version of NodeJS available on docker hub ( https://hub.docker/com/_/node ). Next add these lines to your Dockerfile : WORKDIR /app COPY indexjs . CMD [\"node\", \"indexjs\"] The first line sets the working directory for our container inside which all subsequent commands will be executed - in this case it's set to /app . The second line copies our NodeJS web server code into this directory while the third line tells docker what command it should execute when running our container (in this case it's running our NodeJS web server). Now that we have written out instructions for building our image save them in your Dockerfile . We can now build our image using the following command: docker build -t my_image . This command tells docker to build an image called \u201cmy_image\u201d based on instructions contained within the current directory (denoted by \u201c . \u201d at the end). If successful you should see output similar to this: Successfully built <image_id> Successfully tagged my_image Conculsion","title":"Introduction to Docker and Building Docker Images"},{"location":"containers/docker-100/#introduction-to-docker-and-building-docker-images","text":"Introduction to Docker and Building Docker Images What is Docker? Benefits of Using Docker Prerequisites Exercise Conculsion Docker is a popular open-source platform for developing, shipping, and running applications. It provides an easy way to package applications into containers that can be run on any system with the Docker Engine installed. This lab will provide an introduction to Docker and guide you through the process of building your own Docker images.","title":"Introduction to Docker and Building Docker Images"},{"location":"containers/docker-100/#what-is-docker","text":"Docker is a containerization platform that allows you to package your application into a container image. A container image is a lightweight, stand-alone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. Containers are isolated from each other and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines.","title":"What is Docker?"},{"location":"containers/docker-100/#benefits-of-using-docker","text":"Portability: You can easily move your application from one environment to another without having to worry about compatibility issues or dependencies. Scalability: You can quickly scale up or down your application depending on the load it needs to handle. Security: Containers provide an additional layer of security as they are isolated from each other and the host operating system.","title":"Benefits of Using Docker"},{"location":"containers/docker-100/#prerequisites","text":"Before you begin this lab you should have some basic understanding of command line tools such as docker , docker-compose and docker-machine . Additionally, you should have access to a machine with the latest version of Docker installed on it.","title":"Prerequisites"},{"location":"containers/docker-100/#exercise","text":"In this exercise, we will build our first docker image using the docker build command. We will use a simple NodeJS web server as our example application for this exercise: const http = require(\"http\"); const hostname = \"127.0.0.1\"; const port = 3000; const server = http.createServer((req, res) => { res.statusCode = 200; res.setHeader(\"Content-Type\", \"text/plain\"); res.end(\"Hello World\\n\"); }); server.listen(port, hostname, () => { console.log(`Server running at http://${hostname}:${port}/`); }); To build our image we will need two files - a Dockerfile which contains instructions for building our image and an index.js file which contains our NodeJS web server code (as shown above). Create these two files in the same directory on your Linux machine then open up the Dockerfile in your text editor of choice (e.g., vim). Add the following lines to it: FROM node:latest This line tells docker which base image we want to use for our own custom image - in this case we are using the latest version of NodeJS available on docker hub ( https://hub.docker/com/_/node ). Next add these lines to your Dockerfile : WORKDIR /app COPY indexjs . CMD [\"node\", \"indexjs\"] The first line sets the working directory for our container inside which all subsequent commands will be executed - in this case it's set to /app . The second line copies our NodeJS web server code into this directory while the third line tells docker what command it should execute when running our container (in this case it's running our NodeJS web server). Now that we have written out instructions for building our image save them in your Dockerfile . We can now build our image using the following command: docker build -t my_image . This command tells docker to build an image called \u201cmy_image\u201d based on instructions contained within the current directory (denoted by \u201c . \u201d at the end). If successful you should see output similar to this: Successfully built <image_id> Successfully tagged my_image","title":"Exercise"},{"location":"containers/docker-100/#conculsion","text":"","title":"Conculsion"},{"location":"containers/kube-labs/","text":"Kube Labs Kube Labs Create a Kubernetes cluster on a cloud platform Deploy an application to the Kubernetes cluster Monitor the Kubernetes cluster Automate deployment of applications to the Kubernetes cluster Configure logging for the Kubernetes cluster Configure backups of data stored in the Kubernetes cluster Maintain databases in the cloud Perform regular maintenance tasks on databases Implement semantic versioning for applications deployed in cloud environments Implement GitOps workflows for applications deployed in cloud environments Create a Kubernetes cluster on a cloud platform Create a Kubernetes cluster on a cloud platform using Terraform. Ensure that the cluster is configured to use best practices for security and scalability. Deploy an application to the Kubernetes cluster Deploy an application to the Kubernetes cluster using Terraform. Ensure that the application is configured to use best practices for security and scalability. Monitor the Kubernetes cluster Monitor the Kubernetes cluster using Prometheus and Grafana. Ensure that all metrics are being collected and monitored in real time. Automate deployment of applications to the Kubernetes cluster Automate deployment of applications to the Kubernetes cluster using Jenkins or other CI/CD tools. Ensure that all deployments are done securely and consistently. Configure logging for the Kubernetes cluster Configure logging for the Kubernetes cluster using ELK stack or other logging solutions. Ensure that all logs are being collected and stored securely. Configure backups of data stored in the Kubernetes cluster Configure backups of data stored in the Kubernetes cluster using tools such as Velero or other backup solutions. Ensure that all backups are done securely and regularly scheduled according to best practices. Maintain databases in the cloud Maintain databases in the cloud using tools such as RDS or other database solutions. Ensure that all databases are configured with best practices for security and performance optimization. Perform regular maintenance tasks on databases Perform regular maintenance tasks on databases such as indexing, defragmentation, etc., using tools such as SQL Server Management Studio or other database management tools. Ensure that all maintenance tasks are done according to best practices for security and performance optimization Implement semantic versioning for applications deployed in cloud environments Implement semantic versioning for applications deployed in cloud environments using version control systems such as Git or other version control solutions. Ensure that all versions are tracked accurately and consistently according to best practices Implement GitOps workflows for applications deployed in cloud environments Implement GitOps workflows for applications deployed in cloud environments using tools such as Jenkins X or other CI/CD solutions. Ensure that all deployments are done securely and consistently according to best practices","title":"Kube Labs"},{"location":"containers/kube-labs/#kube-labs","text":"Kube Labs Create a Kubernetes cluster on a cloud platform Deploy an application to the Kubernetes cluster Monitor the Kubernetes cluster Automate deployment of applications to the Kubernetes cluster Configure logging for the Kubernetes cluster Configure backups of data stored in the Kubernetes cluster Maintain databases in the cloud Perform regular maintenance tasks on databases Implement semantic versioning for applications deployed in cloud environments Implement GitOps workflows for applications deployed in cloud environments","title":"Kube Labs"},{"location":"containers/kube-labs/#create-a-kubernetes-cluster-on-a-cloud-platform","text":"Create a Kubernetes cluster on a cloud platform using Terraform. Ensure that the cluster is configured to use best practices for security and scalability.","title":"Create a Kubernetes cluster on a cloud platform"},{"location":"containers/kube-labs/#deploy-an-application-to-the-kubernetes-cluster","text":"Deploy an application to the Kubernetes cluster using Terraform. Ensure that the application is configured to use best practices for security and scalability.","title":"Deploy an application to the Kubernetes cluster"},{"location":"containers/kube-labs/#monitor-the-kubernetes-cluster","text":"Monitor the Kubernetes cluster using Prometheus and Grafana. Ensure that all metrics are being collected and monitored in real time.","title":"Monitor the Kubernetes cluster"},{"location":"containers/kube-labs/#automate-deployment-of-applications-to-the-kubernetes-cluster","text":"Automate deployment of applications to the Kubernetes cluster using Jenkins or other CI/CD tools. Ensure that all deployments are done securely and consistently.","title":"Automate deployment of applications to the Kubernetes cluster"},{"location":"containers/kube-labs/#configure-logging-for-the-kubernetes-cluster","text":"Configure logging for the Kubernetes cluster using ELK stack or other logging solutions. Ensure that all logs are being collected and stored securely.","title":"Configure logging for the Kubernetes cluster"},{"location":"containers/kube-labs/#configure-backups-of-data-stored-in-the-kubernetes-cluster","text":"Configure backups of data stored in the Kubernetes cluster using tools such as Velero or other backup solutions. Ensure that all backups are done securely and regularly scheduled according to best practices.","title":"Configure backups of data stored in the Kubernetes cluster"},{"location":"containers/kube-labs/#maintain-databases-in-the-cloud","text":"Maintain databases in the cloud using tools such as RDS or other database solutions. Ensure that all databases are configured with best practices for security and performance optimization.","title":"Maintain databases in the cloud"},{"location":"containers/kube-labs/#perform-regular-maintenance-tasks-on-databases","text":"Perform regular maintenance tasks on databases such as indexing, defragmentation, etc., using tools such as SQL Server Management Studio or other database management tools. Ensure that all maintenance tasks are done according to best practices for security and performance optimization","title":"Perform regular maintenance tasks on databases"},{"location":"containers/kube-labs/#implement-semantic-versioning-for-applications-deployed-in-cloud-environments","text":"Implement semantic versioning for applications deployed in cloud environments using version control systems such as Git or other version control solutions. Ensure that all versions are tracked accurately and consistently according to best practices","title":"Implement semantic versioning for applications deployed in cloud environments"},{"location":"containers/kube-labs/#implement-gitops-workflows-for-applications-deployed-in-cloud-environments","text":"Implement GitOps workflows for applications deployed in cloud environments using tools such as Jenkins X or other CI/CD solutions. Ensure that all deployments are done securely and consistently according to best practices","title":"Implement GitOps workflows for applications deployed in cloud environments"},{"location":"devops/CICD100/","text":"CI/CD Labs Task 1: Implement In-Place Deployment Title: Implement In-Place Deployment for a Web Application Goal: Learn to perform an in-place update, where the application is updated on the existing infrastructure without changing the environment. Description: Use a simple web application to implement an in-place deployment strategy. Automate the deployment process using a CI/CD tool (e.g., Jenkins, GitLab CI). Acceptance Criteria: Successful update of the application on the same infrastructure without downtime during off-peak hours. Documentation of rollback strategy in case of deployment failure. Task 2: Blue/Green Deployment Title: Execute a Blue/Green Deployment Strategy Goal: Understand and implement blue/green deployments to reduce downtime and risk by running two identical environments. Description: Set up two identical environments: Blue (current production) and Green (new version). Automate the switch from Blue to Green environment using a CI/CD pipeline once the new version is fully tested and ready. Acceptance Criteria: Zero downtime during the switch from Blue to Green environment. Validation tests confirm that the Green environment is functioning as expected before the switch. Documentation of the process, including how to roll back to the Blue environment if issues arise. Task 3: Canary Releases Title: Implement Canary Releases Goal: Learn to gradually roll out changes to a small subset of users before a full rollout, to minimize risk. Description: Modify the CI/CD pipeline to support releasing updates to a small percentage of the total user base. Increase the rollout percentage based on monitoring and feedback. Acceptance Criteria: Successful deployment of the new version to a limited user base without affecting the entire application. Monitoring in place to capture feedback and performance of the new release. Criteria defined for increasing the rollout percentage and for rollback if necessary. Task 4: A/B Testing Deployment Title: Set Up A/B Testing Deployment Goal: Implement A/B testing to compare two versions of an application and analyze user engagement and performance. Description: Create two versions of a feature within the application. Use a CI/CD tool to deploy both versions to a portion of the user base. Collect and analyze metrics to determine the better-performing version. Acceptance Criteria: Both versions of the feature are deployed simultaneously without affecting the overall application performance. Data collection and analysis setup to evaluate the performance of both versions. Documentation on decision-making based on collected data and subsequent actions. Task 5: Automated Testing Integration Title: Integrate Automated Testing in CI/CD Pipeline Goal: Incorporate various automated testing stages (unit, integration, and end-to-end tests) into the CI/CD pipeline to ensure code quality and reliability. Description: Define and implement automated tests for a web application. Integrate these tests into the CI/CD pipeline, ensuring they run at appropriate stages. Acceptance Criteria: Successful integration of automated tests into the CI/CD pipeline. Tests are automatically triggered at each code commit and prior to deployments. Test results are documented, and failures halt the deployment process until resolved. Task 6: Infrastructure as Code (IaC) for CI/CD Setup Title: Automate CI/CD Infrastructure Setup using IaC Goal: Use Infrastructure as Code to automate the setup and teardown of CI/CD pipeline infrastructure. Description: Utilize an IaC tool (e.g., Terraform, CloudFormation) to define the infrastructure required for a CI/CD pipeline. Automate the provisioning and de-provisioning of resources for CI/CD workflows. Acceptance Criteria: Infrastructure for the CI/CD pipeline is provisioned automatically using IaC scripts. Documentation on how to use the IaC scripts to recreate the infrastructure. Ensure the teardown process cleans up resources to avoid unnecessary costs.","title":"CI/CD Labs"},{"location":"devops/CICD100/#cicd-labs","text":"","title":"CI/CD Labs"},{"location":"devops/CICD100/#task-1-implement-in-place-deployment","text":"Title: Implement In-Place Deployment for a Web Application Goal: Learn to perform an in-place update, where the application is updated on the existing infrastructure without changing the environment. Description: Use a simple web application to implement an in-place deployment strategy. Automate the deployment process using a CI/CD tool (e.g., Jenkins, GitLab CI). Acceptance Criteria: Successful update of the application on the same infrastructure without downtime during off-peak hours. Documentation of rollback strategy in case of deployment failure.","title":"Task 1: Implement In-Place Deployment"},{"location":"devops/CICD100/#task-2-bluegreen-deployment","text":"Title: Execute a Blue/Green Deployment Strategy Goal: Understand and implement blue/green deployments to reduce downtime and risk by running two identical environments. Description: Set up two identical environments: Blue (current production) and Green (new version). Automate the switch from Blue to Green environment using a CI/CD pipeline once the new version is fully tested and ready. Acceptance Criteria: Zero downtime during the switch from Blue to Green environment. Validation tests confirm that the Green environment is functioning as expected before the switch. Documentation of the process, including how to roll back to the Blue environment if issues arise.","title":"Task 2: Blue/Green Deployment"},{"location":"devops/CICD100/#task-3-canary-releases","text":"Title: Implement Canary Releases Goal: Learn to gradually roll out changes to a small subset of users before a full rollout, to minimize risk. Description: Modify the CI/CD pipeline to support releasing updates to a small percentage of the total user base. Increase the rollout percentage based on monitoring and feedback. Acceptance Criteria: Successful deployment of the new version to a limited user base without affecting the entire application. Monitoring in place to capture feedback and performance of the new release. Criteria defined for increasing the rollout percentage and for rollback if necessary.","title":"Task 3: Canary Releases"},{"location":"devops/CICD100/#task-4-ab-testing-deployment","text":"Title: Set Up A/B Testing Deployment Goal: Implement A/B testing to compare two versions of an application and analyze user engagement and performance. Description: Create two versions of a feature within the application. Use a CI/CD tool to deploy both versions to a portion of the user base. Collect and analyze metrics to determine the better-performing version. Acceptance Criteria: Both versions of the feature are deployed simultaneously without affecting the overall application performance. Data collection and analysis setup to evaluate the performance of both versions. Documentation on decision-making based on collected data and subsequent actions.","title":"Task 4: A/B Testing Deployment"},{"location":"devops/CICD100/#task-5-automated-testing-integration","text":"Title: Integrate Automated Testing in CI/CD Pipeline Goal: Incorporate various automated testing stages (unit, integration, and end-to-end tests) into the CI/CD pipeline to ensure code quality and reliability. Description: Define and implement automated tests for a web application. Integrate these tests into the CI/CD pipeline, ensuring they run at appropriate stages. Acceptance Criteria: Successful integration of automated tests into the CI/CD pipeline. Tests are automatically triggered at each code commit and prior to deployments. Test results are documented, and failures halt the deployment process until resolved.","title":"Task 5: Automated Testing Integration"},{"location":"devops/CICD100/#task-6-infrastructure-as-code-iac-for-cicd-setup","text":"Title: Automate CI/CD Infrastructure Setup using IaC Goal: Use Infrastructure as Code to automate the setup and teardown of CI/CD pipeline infrastructure. Description: Utilize an IaC tool (e.g., Terraform, CloudFormation) to define the infrastructure required for a CI/CD pipeline. Automate the provisioning and de-provisioning of resources for CI/CD workflows. Acceptance Criteria: Infrastructure for the CI/CD pipeline is provisioned automatically using IaC scripts. Documentation on how to use the IaC scripts to recreate the infrastructure. Ensure the teardown process cleans up resources to avoid unnecessary costs.","title":"Task 6: Infrastructure as Code (IaC) for CI/CD Setup"},{"location":"devops/CICDplatforms/","text":"CI/CD Platforms GitHub Actions Pros Integration: Seamless integration with GitHub repositories, providing native support for CI/CD within the GitHub ecosystem. Community: Access to a marketplace of actions created by the community for common tasks. Cost: Free for public repositories and comes with a generous free tier for private repositories. Cons Limited to GitHub: Tightly coupled with GitHub, not suitable for projects hosted elsewhere. Complexity: Complex workflows can become difficult to manage, especially for larger projects. Resource Limits: Usage limits on private repositories can be restrictive for larger teams or projects. Jenkins Pros Flexibility: Highly configurable with a vast plugin ecosystem to extend functionality. Community Support: Large community and extensive documentation. Platform Agnostic: Can be used with any technology and cloud platform. Cons Complexity: Can be complex to set up and manage, especially at scale. UI/UX: The user interface is not as modern or intuitive as some newer tools. Maintenance: Self-hosted instances require regular maintenance. Security: Java apps are prone to vulnerabilities. GitLab CI/CD Pros Integrated Solution: Comprehensive DevOps platform with built-in CI/CD, reducing the need for additional tools. Ease of Use: Intuitive YAML-based configuration for pipelines. Scalability: Scales well with the GitLab infrastructure, supporting high availability. Cons Resource Intensive: Can be resource-heavy, especially for self-hosted instances. Learning Curve: While GitLab CI/CD is user-friendly, mastering its full suite of features can take time. Integration: While it integrates well within the GitLab ecosystem, integration with external tools can be less seamless. Travis CI Pros Ease of Setup: Simple to set up with GitHub projects, often requiring minimal configuration. Free Tier: Offers a free tier for public repositories, making it accessible for open-source projects. Configuration: Builds are configured using YAML files, allowing for version-controlled configuration. Cons Limited Free Tier: Limited build minutes for private repositories on the free tier. Performance: Build start times can be slow during peak usage. Customization: Less flexible compared to tools like Jenkins in terms of customization and extensibility. CircleCI Pros Performance: Fast build times and efficient handling of parallel jobs. Configuration: Uses YAML for pipeline definitions, supporting complex workflows. Integration: Strong integration with GitHub and Bitbucket. Cons Pricing: Can become expensive for teams with high usage. Limited Customization: While flexible, it may not offer the same level of customization as Jenkins. Complexity: Complex workflows can lead to intricate configuration files.","title":"CI/CD Platforms"},{"location":"devops/CICDplatforms/#cicd-platforms","text":"","title":"CI/CD Platforms"},{"location":"devops/CICDplatforms/#github-actions","text":"","title":"GitHub Actions"},{"location":"devops/CICDplatforms/#pros","text":"Integration: Seamless integration with GitHub repositories, providing native support for CI/CD within the GitHub ecosystem. Community: Access to a marketplace of actions created by the community for common tasks. Cost: Free for public repositories and comes with a generous free tier for private repositories.","title":"Pros"},{"location":"devops/CICDplatforms/#cons","text":"Limited to GitHub: Tightly coupled with GitHub, not suitable for projects hosted elsewhere. Complexity: Complex workflows can become difficult to manage, especially for larger projects. Resource Limits: Usage limits on private repositories can be restrictive for larger teams or projects.","title":"Cons"},{"location":"devops/CICDplatforms/#jenkins","text":"","title":"Jenkins"},{"location":"devops/CICDplatforms/#pros_1","text":"Flexibility: Highly configurable with a vast plugin ecosystem to extend functionality. Community Support: Large community and extensive documentation. Platform Agnostic: Can be used with any technology and cloud platform.","title":"Pros"},{"location":"devops/CICDplatforms/#cons_1","text":"Complexity: Can be complex to set up and manage, especially at scale. UI/UX: The user interface is not as modern or intuitive as some newer tools. Maintenance: Self-hosted instances require regular maintenance. Security: Java apps are prone to vulnerabilities.","title":"Cons"},{"location":"devops/CICDplatforms/#gitlab-cicd","text":"","title":"GitLab CI/CD"},{"location":"devops/CICDplatforms/#pros_2","text":"Integrated Solution: Comprehensive DevOps platform with built-in CI/CD, reducing the need for additional tools. Ease of Use: Intuitive YAML-based configuration for pipelines. Scalability: Scales well with the GitLab infrastructure, supporting high availability.","title":"Pros"},{"location":"devops/CICDplatforms/#cons_2","text":"Resource Intensive: Can be resource-heavy, especially for self-hosted instances. Learning Curve: While GitLab CI/CD is user-friendly, mastering its full suite of features can take time. Integration: While it integrates well within the GitLab ecosystem, integration with external tools can be less seamless.","title":"Cons"},{"location":"devops/CICDplatforms/#travis-ci","text":"","title":"Travis CI"},{"location":"devops/CICDplatforms/#pros_3","text":"Ease of Setup: Simple to set up with GitHub projects, often requiring minimal configuration. Free Tier: Offers a free tier for public repositories, making it accessible for open-source projects. Configuration: Builds are configured using YAML files, allowing for version-controlled configuration.","title":"Pros"},{"location":"devops/CICDplatforms/#cons_3","text":"Limited Free Tier: Limited build minutes for private repositories on the free tier. Performance: Build start times can be slow during peak usage. Customization: Less flexible compared to tools like Jenkins in terms of customization and extensibility.","title":"Cons"},{"location":"devops/CICDplatforms/#circleci","text":"","title":"CircleCI"},{"location":"devops/CICDplatforms/#pros_4","text":"Performance: Fast build times and efficient handling of parallel jobs. Configuration: Uses YAML for pipeline definitions, supporting complex workflows. Integration: Strong integration with GitHub and Bitbucket.","title":"Pros"},{"location":"devops/CICDplatforms/#cons_4","text":"Pricing: Can become expensive for teams with high usage. Limited Customization: While flexible, it may not offer the same level of customization as Jenkins. Complexity: Complex workflows can lead to intricate configuration files.","title":"Cons"},{"location":"devops/ConfigMgmt/","text":"Config Mgmt Ansible TBD Playbooks CIS Best Practices - https://github.com/orgs/ansible-lockdown/repositories","title":"Config Mgmt"},{"location":"devops/ConfigMgmt/#config-mgmt","text":"","title":"Config Mgmt"},{"location":"devops/ConfigMgmt/#ansible","text":"TBD","title":"Ansible"},{"location":"devops/ConfigMgmt/#playbooks","text":"CIS Best Practices - https://github.com/orgs/ansible-lockdown/repositories","title":"Playbooks"},{"location":"devops/Immutable/","text":"Immutable An immutable infrastructure is another infrastructure paradigm in which servers are never modified after they\u2019re deployed. If something needs to be updated, fixed, or modified in any way, new servers built from a common image with the appropriate changes are provisioned to replace the old ones. After they\u2019re validated, they\u2019re put into use and the old ones are decommissioned. Packer","title":"Immutable"},{"location":"devops/Immutable/#immutable","text":"An immutable infrastructure is another infrastructure paradigm in which servers are never modified after they\u2019re deployed. If something needs to be updated, fixed, or modified in any way, new servers built from a common image with the appropriate changes are provisioned to replace the old ones. After they\u2019re validated, they\u2019re put into use and the old ones are decommissioned. Packer","title":"Immutable"},{"location":"devops/Packer/","text":"Introduction to Immutable Architectures and Packer Immutable architectures are becoming increasingly popular as a way to deploy applications and services in the cloud. By using immutable architectures, you can ensure that your applications and services are always running the same version of code, with no manual intervention required. This makes it easier to manage deployments, as well as making them more secure. In this lab, we will explore the benefits of immutable architectures, and how to use Packer to build Amazon Machine Images (AMIs). What is an Immutable Architecture? An immutable architecture is a type of software architecture where components are not modified after they have been deployed. This means that once an application or service has been deployed, it cannot be changed or updated in any way. Instead, any changes must be made by deploying a new version of the application or service. This ensures that all components are always running the same version of code, which makes it easier to manage deployments and keep them secure. What is Packer? Packer is an open source tool for creating machine images for multiple platforms from a single source configuration. It automates the process of creating machine images for different cloud providers such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), DigitalOcean and more. Packer can be used to build AMIs for AWS EC2 instances, which can then be used to create new instances with pre-configured settings and software packages installed. Lab Exercise: Building an AMI with Packer In this lab exercise we will use Packer to build an AMI for an AWS EC2 instance. We will use a simple configuration file written in JSON format to define our image settings and software packages that need to be installed on the instance. Step 1: Install Packer The first step is to install Packer on your local machine. You can download the latest version from here . Once you have downloaded the package, follow the instructions provided by Packer's documentation to install it on your system. Step 2: Create Configuration File Once you have installed Packer on your system, create a configuration file in JSON format that defines all of the settings for your image and any software packages that need to be installed on it. Here is an example configuration file: { \"builders\": [ { \"type\": \"amazon-ebs\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"amzn2-ami-hvm*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"amazon\" ], \"most_recent\": true }, } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \u201csudo apt update\u201d, \u201csudo apt install -y nginx\u201d ] } ] } This configuration file tells Packer that we want it to create an Amazon EBS image in us-east-1 region using the most recent HVM AMI available from Amazon (specified by source_ami_filter ). It also tells Packer that we want it to install Nginx web server on our image (specified by provisioners ). Once you have created your configuration file save it as packer_configuration.json . Step 3: Build Image with Packer Now that you have created your configuration file you can use Packer's command line interface (CLI) tool packer to build your image from it: packer build packer_configuration.json This command will tell packer to read our configuration file packer_configuration and create our image according to its settings specified in the file . Once packers finishes building our image ,it will output its ID which we can use later when creating EC2 instances from our newly created AMI .","title":"Introduction to Immutable Architectures and Packer"},{"location":"devops/Packer/#introduction-to-immutable-architectures-and-packer","text":"Immutable architectures are becoming increasingly popular as a way to deploy applications and services in the cloud. By using immutable architectures, you can ensure that your applications and services are always running the same version of code, with no manual intervention required. This makes it easier to manage deployments, as well as making them more secure. In this lab, we will explore the benefits of immutable architectures, and how to use Packer to build Amazon Machine Images (AMIs).","title":"Introduction to Immutable Architectures and Packer"},{"location":"devops/Packer/#what-is-an-immutable-architecture","text":"An immutable architecture is a type of software architecture where components are not modified after they have been deployed. This means that once an application or service has been deployed, it cannot be changed or updated in any way. Instead, any changes must be made by deploying a new version of the application or service. This ensures that all components are always running the same version of code, which makes it easier to manage deployments and keep them secure.","title":"What is an Immutable Architecture?"},{"location":"devops/Packer/#what-is-packer","text":"Packer is an open source tool for creating machine images for multiple platforms from a single source configuration. It automates the process of creating machine images for different cloud providers such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), DigitalOcean and more. Packer can be used to build AMIs for AWS EC2 instances, which can then be used to create new instances with pre-configured settings and software packages installed.","title":"What is Packer?"},{"location":"devops/Packer/#lab-exercise-building-an-ami-with-packer","text":"In this lab exercise we will use Packer to build an AMI for an AWS EC2 instance. We will use a simple configuration file written in JSON format to define our image settings and software packages that need to be installed on the instance.","title":"Lab Exercise: Building an AMI with Packer"},{"location":"devops/Packer/#step-1-install-packer","text":"The first step is to install Packer on your local machine. You can download the latest version from here . Once you have downloaded the package, follow the instructions provided by Packer's documentation to install it on your system.","title":"Step 1: Install Packer"},{"location":"devops/Packer/#step-2-create-configuration-file","text":"Once you have installed Packer on your system, create a configuration file in JSON format that defines all of the settings for your image and any software packages that need to be installed on it. Here is an example configuration file: { \"builders\": [ { \"type\": \"amazon-ebs\", \"region\": \"us-east-1\", \"source_ami_filter\": { \"filters\": { \"virtualization-type\": \"hvm\", \"name\": \"amzn2-ami-hvm*\", \"root-device-type\": \"ebs\" }, \"owners\": [ \"amazon\" ], \"most_recent\": true }, } ], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \u201csudo apt update\u201d, \u201csudo apt install -y nginx\u201d ] } ] } This configuration file tells Packer that we want it to create an Amazon EBS image in us-east-1 region using the most recent HVM AMI available from Amazon (specified by source_ami_filter ). It also tells Packer that we want it to install Nginx web server on our image (specified by provisioners ). Once you have created your configuration file save it as packer_configuration.json .","title":"Step 2: Create Configuration File"},{"location":"devops/Packer/#step-3-build-image-with-packer","text":"Now that you have created your configuration file you can use Packer's command line interface (CLI) tool packer to build your image from it: packer build packer_configuration.json This command will tell packer to read our configuration file packer_configuration and create our image according to its settings specified in the file . Once packers finishes building our image ,it will output its ID which we can use later when creating EC2 instances from our newly created AMI .","title":"Step 3: Build Image with Packer"}]}